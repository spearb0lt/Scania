{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb9945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "# import os\n",
    "# import sys\n",
    "# import json\n",
    "# import time\n",
    "# import h5py\n",
    "# import joblib\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from datetime import datetime\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "# from torch.optim import Adam\n",
    "# from torch.nn import MSELoss\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# # === Adjust imports based on your project structure ===\n",
    "# # from running_tf+specdp import CombinedRULModel, RULCombinedDataset, load_from_h5\n",
    "\n",
    "\n",
    "\n",
    "# H5_PATH = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with h5py.File(H5_PATH, \"r\") as f:\n",
    "#         X = f[\"X_windows\"][:]\n",
    "#         y = f[\"y_labels\"][:]\n",
    "#         vids = f[\"window_vids\"][:]\n",
    "#         specs = f[\"specs_per_window\"][:]\n",
    "\n",
    "# class RULCombinedDataset(Dataset):\n",
    "#     def __init__(self, X, specs, y):\n",
    "#         self.X = X\n",
    "#         self.specs = specs\n",
    "#         self.y = y.reshape(-1, 1)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         return (\n",
    "#             torch.from_numpy(self.specs[i]).long(),\n",
    "#             torch.from_numpy(self.X[i]).float(),\n",
    "#             torch.from_numpy(self.y[i]).float()\n",
    "#         )\n",
    "\n",
    "# class TimeSeriesEmbedder(nn.Module):\n",
    "#     def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(num_features, d_model)\n",
    "#         enc_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.input_proj(x)\n",
    "#         x = self.encoder(x)\n",
    "#         return x[:, -1, :]\n",
    "# class CombinedRULModel(nn.Module):\n",
    "#     def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "#         super().__init__()\n",
    "#         self.tf = TimeSeriesEmbedder(num_sensor_features, continuous_dim)\n",
    "#         if cont_mean_std is None:\n",
    "#             cont_mean_std = torch.stack([torch.zeros(continuous_dim), torch.ones(continuous_dim)], dim=1)\n",
    "#         self.tab = TabTransformer(\n",
    "#             categories=categories,\n",
    "#             num_continuous=continuous_dim,\n",
    "#             dim=continuous_dim,\n",
    "#             dim_out=1,\n",
    "#             depth=6,\n",
    "#             heads=8,\n",
    "#             attn_dropout=0.1,\n",
    "#             ff_dropout=0.1,\n",
    "#             mlp_hidden_mults=(4,2),\n",
    "#             mlp_act=nn.ReLU(),\n",
    "#             continuous_mean_std=cont_mean_std\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_cat, x_ts):\n",
    "#         cont = self.tf(x_ts)\n",
    "#         return self.tab(x_cat, cont)\n",
    "\n",
    "# # C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-DP-20250618_160955\\checkpoint.pth\n",
    "\n",
    "# # C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-NDP-20250616_163702\\checkpoint.pth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === Paths (adjust as needed) ===\n",
    "# ENCODER_PATH = r\"C:/Users/ASUS/Desktop/SCANIA/2024-34-2/2024-34-2/Important_script/spec_encoder.joblib\"\n",
    "# CHECKPOINT_PATH = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-DP-20250618_160955\\checkpoint.pth\"  # <-- UPDATE with actual\n",
    "# USE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # === Load Data ===\n",
    "# # X, y, _, specs = load_from_h5()\n",
    "# X_mem, X_nonmem, y_mem, y_nonmem, specs_mem, specs_nonmem = train_test_split(\n",
    "#     X, y, specs, test_size=0.5, random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # === Load Model ===\n",
    "# encoder = joblib.load(ENCODER_PATH)\n",
    "# cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "# model = CombinedRULModel(\n",
    "#     num_sensor_features=X.shape[2],\n",
    "#     context_length=X.shape[1],\n",
    "#     categories=cat_sizes,\n",
    "#     continuous_dim=128\n",
    "# ).to(USE_DEVICE)\n",
    "\n",
    "# model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=USE_DEVICE))\n",
    "# model.eval()\n",
    "\n",
    "# # === Collect Per-Sample MSE ===\n",
    "# def get_losses(X, specs, y):\n",
    "#     dataset = RULCombinedDataset(X, specs, y)\n",
    "#     loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "#     losses = []\n",
    "#     with torch.no_grad():\n",
    "#         for xc, xt, yb in loader:\n",
    "#             xc, xt, yb = xc.to(USE_DEVICE), xt.to(USE_DEVICE), yb.to(USE_DEVICE)\n",
    "#             preds = model(xc, xt)\n",
    "#             loss = ((preds - yb) ** 2).squeeze().cpu().numpy()\n",
    "#             losses.append(loss)\n",
    "#     return np.concatenate(losses).reshape(-1, 1)\n",
    "\n",
    "# feat_mem = get_losses(X_mem, specs_mem, y_mem)\n",
    "# feat_non = get_losses(X_nonmem, specs_nonmem, y_nonmem)\n",
    "\n",
    "# X_attack = np.vstack([feat_mem, feat_non])\n",
    "# y_attack = np.concatenate([np.ones(len(feat_mem)), np.zeros(len(feat_non))])\n",
    "\n",
    "# Xa_train, Xa_test, ya_train, ya_test = train_test_split(\n",
    "#     X_attack, y_attack, test_size=0.3, random_state=0, stratify=y_attack\n",
    "# )\n",
    "\n",
    "#  # === Train Attack Model ===\n",
    "# attack_model = LogisticRegression(solver=\"lbfgs\")\n",
    "# attack_model.fit(Xa_train, ya_train)\n",
    "\n",
    "# # === Evaluate Attack ===\n",
    "# ya_proba = attack_model.predict_proba(Xa_test)[:, 1]\n",
    "# ya_preds = attack_model.predict(Xa_test)\n",
    "\n",
    "# auc = roc_auc_score(ya_test, ya_proba)\n",
    "# acc = accuracy_score(ya_test, ya_preds)\n",
    "\n",
    "# print(f\"✅ MIA Success Rate (AUC): {auc * 100:.2f}%\")\n",
    "# print(f\"✅ MIA Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e134ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "npx https://github.com/google-gemini/gemini-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2628b",
   "metadata": {},
   "source": [
    "# NEEDED MIA ATTACK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de3e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import h5py\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# === Adjust imports based on your project structure ===\n",
    "# from running_tf+specdp import CombinedRULModel, RULCombinedDataset, load_from_h5\n",
    "\n",
    "\n",
    "\n",
    "H5_PATH = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(H5_PATH, \"r\") as f:\n",
    "        X = f[\"X_windows\"][:]\n",
    "        y = f[\"y_labels\"][:]\n",
    "        vids = f[\"window_vids\"][:]\n",
    "        specs = f[\"specs_per_window\"][:]\n",
    "\n",
    "class RULCombinedDataset(Dataset):\n",
    "    def __init__(self, X, specs, y):\n",
    "        self.X = X\n",
    "        self.specs = specs\n",
    "        self.y = y.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.from_numpy(self.specs[i]).long(),\n",
    "            torch.from_numpy(self.X[i]).float(),\n",
    "            torch.from_numpy(self.y[i]).float()\n",
    "        )\n",
    "\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        return x[:, -1, :]\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "        super().__init__()\n",
    "        self.tf = TimeSeriesEmbedder(num_sensor_features, continuous_dim)\n",
    "        if cont_mean_std is None:\n",
    "            cont_mean_std = torch.stack([torch.zeros(continuous_dim), torch.ones(continuous_dim)], dim=1)\n",
    "        self.tab = TabTransformer(\n",
    "            categories=categories,\n",
    "            num_continuous=continuous_dim,\n",
    "            dim=continuous_dim,\n",
    "            dim_out=1,\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4,2),\n",
    "            mlp_act=nn.ReLU(),\n",
    "            continuous_mean_std=cont_mean_std\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_ts):\n",
    "        cont = self.tf(x_ts)\n",
    "        return self.tab(x_cat, cont)\n",
    "\n",
    "# C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-DP-20250618_160955\\checkpoint.pth\n",
    "\n",
    "# C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-NDP-20250616_163702\\checkpoint.pth\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92718d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Paths (adjust as needed) ===\n",
    "ENCODER_PATH = r\"C:/Users/ASUS/Desktop/SCANIA/2024-34-2/2024-34-2/Important_script/spec_encoder.joblib\"\n",
    "CHECKPOINT_PATH = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-DP-20250618_160955\\checkpoint.pth\"  # <-- UPDATE with actual\n",
    "# CHECKPOINT_PATH = r\"C:\\Users\\ASUS\\Downloads\\best_dp_model_adaptive_largeralpha.pt\"  # <-- UPDATE with actual\n",
    "\n",
    "USE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load Data ===\n",
    "# X, y, _, specs = load_from_h5()\n",
    "X_mem, X_nonmem, y_mem, y_nonmem, specs_mem, specs_nonmem = train_test_split(\n",
    "    X, y, specs, test_size=0.5, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d85df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:440: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.3.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CombinedRULModel:\n\tMissing key(s) in state_dict: \"tf.input_proj.weight\", \"tf.input_proj.bias\", \"tf.encoder.layers.0.self_attn.in_proj_weight\", \"tf.encoder.layers.0.self_attn.in_proj_bias\", \"tf.encoder.layers.0.self_attn.out_proj.weight\", \"tf.encoder.layers.0.self_attn.out_proj.bias\", \"tf.encoder.layers.0.linear1.weight\", \"tf.encoder.layers.0.linear1.bias\", \"tf.encoder.layers.0.linear2.weight\", \"tf.encoder.layers.0.linear2.bias\", \"tf.encoder.layers.0.norm1.weight\", \"tf.encoder.layers.0.norm1.bias\", \"tf.encoder.layers.0.norm2.weight\", \"tf.encoder.layers.0.norm2.bias\", \"tf.encoder.layers.1.self_attn.in_proj_weight\", \"tf.encoder.layers.1.self_attn.in_proj_bias\", \"tf.encoder.layers.1.self_attn.out_proj.weight\", \"tf.encoder.layers.1.self_attn.out_proj.bias\", \"tf.encoder.layers.1.linear1.weight\", \"tf.encoder.layers.1.linear1.bias\", \"tf.encoder.layers.1.linear2.weight\", \"tf.encoder.layers.1.linear2.bias\", \"tf.encoder.layers.1.norm1.weight\", \"tf.encoder.layers.1.norm1.bias\", \"tf.encoder.layers.1.norm2.weight\", \"tf.encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"ts.proj.weight\", \"ts.proj.bias\", \"ts.enc.layers.0.self_attn.in_proj_weight\", \"ts.enc.layers.0.self_attn.in_proj_bias\", \"ts.enc.layers.0.self_attn.out_proj.weight\", \"ts.enc.layers.0.self_attn.out_proj.bias\", \"ts.enc.layers.0.linear1.weight\", \"ts.enc.layers.0.linear1.bias\", \"ts.enc.layers.0.linear2.weight\", \"ts.enc.layers.0.linear2.bias\", \"ts.enc.layers.0.norm1.weight\", \"ts.enc.layers.0.norm1.bias\", \"ts.enc.layers.0.norm2.weight\", \"ts.enc.layers.0.norm2.bias\", \"ts.enc.layers.1.self_attn.in_proj_weight\", \"ts.enc.layers.1.self_attn.in_proj_bias\", \"ts.enc.layers.1.self_attn.out_proj.weight\", \"ts.enc.layers.1.self_attn.out_proj.bias\", \"ts.enc.layers.1.linear1.weight\", \"ts.enc.layers.1.linear1.bias\", \"ts.enc.layers.1.linear2.weight\", \"ts.enc.layers.1.linear2.bias\", \"ts.enc.layers.1.norm1.weight\", \"ts.enc.layers.1.norm1.bias\", \"ts.enc.layers.1.norm2.weight\", \"ts.enc.layers.1.norm2.bias\". \n\tsize mismatch for tab.shared_category_embed: copying a param with shape torch.Size([8, 32]) from checkpoint, the shape in current model is torch.Size([8, 16]).\n\tsize mismatch for tab.continuous_mean_std: copying a param with shape torch.Size([256, 2]) from checkpoint, the shape in current model is torch.Size([128, 2]).\n\tsize mismatch for tab.category_embed.weight: copying a param with shape torch.Size([92, 224]) from checkpoint, the shape in current model is torch.Size([92, 112]).\n\tsize mismatch for tab.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.0.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.0.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.1.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.1.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.2.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.2.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.3.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.3.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.4.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.4.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.5.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.5.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.mlp.mlp.0.weight: copying a param with shape torch.Size([9216, 2304]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for tab.mlp.mlp.0.bias: copying a param with shape torch.Size([9216]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for tab.mlp.mlp.2.weight: copying a param with shape torch.Size([4608, 9216]) from checkpoint, the shape in current model is torch.Size([2304, 4608]).\n\tsize mismatch for tab.mlp.mlp.2.bias: copying a param with shape torch.Size([4608]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for tab.mlp.mlp.4.weight: copying a param with shape torch.Size([1, 4608]) from checkpoint, the shape in current model is torch.Size([1, 2304]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m cat_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m encoder\u001b[38;5;241m.\u001b[39mcategories_)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m CombinedRULModel(\n\u001b[0;32m      6\u001b[0m     num_sensor_features\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      7\u001b[0m     context_length\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      8\u001b[0m     categories\u001b[38;5;241m=\u001b[39mcat_sizes,\n\u001b[0;32m      9\u001b[0m     continuous_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mto(USE_DEVICE)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_DEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CombinedRULModel:\n\tMissing key(s) in state_dict: \"tf.input_proj.weight\", \"tf.input_proj.bias\", \"tf.encoder.layers.0.self_attn.in_proj_weight\", \"tf.encoder.layers.0.self_attn.in_proj_bias\", \"tf.encoder.layers.0.self_attn.out_proj.weight\", \"tf.encoder.layers.0.self_attn.out_proj.bias\", \"tf.encoder.layers.0.linear1.weight\", \"tf.encoder.layers.0.linear1.bias\", \"tf.encoder.layers.0.linear2.weight\", \"tf.encoder.layers.0.linear2.bias\", \"tf.encoder.layers.0.norm1.weight\", \"tf.encoder.layers.0.norm1.bias\", \"tf.encoder.layers.0.norm2.weight\", \"tf.encoder.layers.0.norm2.bias\", \"tf.encoder.layers.1.self_attn.in_proj_weight\", \"tf.encoder.layers.1.self_attn.in_proj_bias\", \"tf.encoder.layers.1.self_attn.out_proj.weight\", \"tf.encoder.layers.1.self_attn.out_proj.bias\", \"tf.encoder.layers.1.linear1.weight\", \"tf.encoder.layers.1.linear1.bias\", \"tf.encoder.layers.1.linear2.weight\", \"tf.encoder.layers.1.linear2.bias\", \"tf.encoder.layers.1.norm1.weight\", \"tf.encoder.layers.1.norm1.bias\", \"tf.encoder.layers.1.norm2.weight\", \"tf.encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"ts.proj.weight\", \"ts.proj.bias\", \"ts.enc.layers.0.self_attn.in_proj_weight\", \"ts.enc.layers.0.self_attn.in_proj_bias\", \"ts.enc.layers.0.self_attn.out_proj.weight\", \"ts.enc.layers.0.self_attn.out_proj.bias\", \"ts.enc.layers.0.linear1.weight\", \"ts.enc.layers.0.linear1.bias\", \"ts.enc.layers.0.linear2.weight\", \"ts.enc.layers.0.linear2.bias\", \"ts.enc.layers.0.norm1.weight\", \"ts.enc.layers.0.norm1.bias\", \"ts.enc.layers.0.norm2.weight\", \"ts.enc.layers.0.norm2.bias\", \"ts.enc.layers.1.self_attn.in_proj_weight\", \"ts.enc.layers.1.self_attn.in_proj_bias\", \"ts.enc.layers.1.self_attn.out_proj.weight\", \"ts.enc.layers.1.self_attn.out_proj.bias\", \"ts.enc.layers.1.linear1.weight\", \"ts.enc.layers.1.linear1.bias\", \"ts.enc.layers.1.linear2.weight\", \"ts.enc.layers.1.linear2.bias\", \"ts.enc.layers.1.norm1.weight\", \"ts.enc.layers.1.norm1.bias\", \"ts.enc.layers.1.norm2.weight\", \"ts.enc.layers.1.norm2.bias\". \n\tsize mismatch for tab.shared_category_embed: copying a param with shape torch.Size([8, 32]) from checkpoint, the shape in current model is torch.Size([8, 16]).\n\tsize mismatch for tab.continuous_mean_std: copying a param with shape torch.Size([256, 2]) from checkpoint, the shape in current model is torch.Size([128, 2]).\n\tsize mismatch for tab.category_embed.weight: copying a param with shape torch.Size([92, 224]) from checkpoint, the shape in current model is torch.Size([92, 112]).\n\tsize mismatch for tab.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.0.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.0.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.0.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.0.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.0.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.1.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.1.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.1.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.1.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.1.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.2.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.2.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.2.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.2.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.2.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.3.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.3.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.3.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.3.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.3.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.4.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.4.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.4.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.4.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.4.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.5.0.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_qkv.weight: copying a param with shape torch.Size([384, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_out.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for tab.transformer.layers.5.0.branch.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.0.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.dynamic_alpha_fn: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([128, 5]).\n\tsize mismatch for tab.transformer.layers.5.1.dynamic_beta_fn: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.0.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for tab.transformer.layers.5.1.branch.fn.net.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.transformer.layers.5.1.norm.gamma: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for tab.mlp.mlp.0.weight: copying a param with shape torch.Size([9216, 2304]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for tab.mlp.mlp.0.bias: copying a param with shape torch.Size([9216]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for tab.mlp.mlp.2.weight: copying a param with shape torch.Size([4608, 9216]) from checkpoint, the shape in current model is torch.Size([2304, 4608]).\n\tsize mismatch for tab.mlp.mlp.2.bias: copying a param with shape torch.Size([4608]) from checkpoint, the shape in current model is torch.Size([2304]).\n\tsize mismatch for tab.mlp.mlp.4.weight: copying a param with shape torch.Size([1, 4608]) from checkpoint, the shape in current model is torch.Size([1, 2304])."
     ]
    }
   ],
   "source": [
    "\n",
    "# === Load Model ===\n",
    "encoder = joblib.load(ENCODER_PATH)\n",
    "cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "model = CombinedRULModel(\n",
    "    num_sensor_features=X.shape[2],\n",
    "    context_length=X.shape[1],\n",
    "    categories=cat_sizes,\n",
    "    continuous_dim=128\n",
    ").to(USE_DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=USE_DEVICE))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352bac8",
   "metadata": {},
   "source": [
    "1. Collecting Per-Sample MSE Losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Feature Losses: (3276, 1)\n",
      "Non-Memory Feature Losses: (3277, 1)\n"
     ]
    }
   ],
   "source": [
    "# _mem->menas taken from my dataset sample\n",
    "#_nonmem->means taken from non-memory dataset sample\n",
    "\n",
    "\n",
    "# === Collect Per-Sample MSE ===\n",
    "def get_losses(X, specs, y):\n",
    "    dataset = RULCombinedDataset(X, specs, y)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for xc, xt, yb in loader:\n",
    "            xc, xt, yb = xc.to(USE_DEVICE), xt.to(USE_DEVICE), yb.to(USE_DEVICE)\n",
    "            preds = model(xc, xt)\n",
    "            loss = ((preds - yb) ** 2).squeeze().cpu().numpy()\n",
    "            losses.append(loss)\n",
    "    return np.concatenate(losses).reshape(-1, 1)\n",
    "\n",
    "feat_mem = get_losses(X_mem, specs_mem, y_mem)\n",
    "feat_non = get_losses(X_nonmem, specs_nonmem, y_nonmem)\n",
    "\n",
    "print(\"Memory Feature Losses:\", feat_mem.shape)\n",
    "print(\"Non-Memory Feature Losses:\", feat_non.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bd6f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Feature Losses: [[283.5414   ]\n",
      " [239.12152  ]\n",
      " [  2.9046319]\n",
      " ...\n",
      " [290.73364  ]\n",
      " [430.3982   ]\n",
      " [186.13582  ]]\n",
      "Non-Memory Feature Losses: [[  29.937943]\n",
      " [ 268.57318 ]\n",
      " [ 112.57375 ]\n",
      " ...\n",
      " [1673.507   ]\n",
      " [  51.18096 ]\n",
      " [ 359.92334 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory Feature Losses:\", feat_mem)\n",
    "print(\"Non-Memory Feature Losses:\", feat_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0129c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24311da",
   "metadata": {},
   "source": [
    "\n",
    "Preparing the Attack Dataset\n",
    "Features (X_attack): stack the member-losses ABOVE the non-member-losses into one array of shape (total_samples, 1).\n",
    "\n",
    "Labels (y_attack):1 for every member sample 0 for every non-member sample\n",
    "\n",
    "stratify=y_attack ensures the same member/non-member ratio in both sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X attack is [[ 283.5414   ]\n",
      " [ 239.12152  ]\n",
      " [   2.9046319]\n",
      " ...\n",
      " [1673.507    ]\n",
      " [  51.18096  ]\n",
      " [ 359.92334  ]]\n",
      "y attack is [1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Prepare Attack Dataset ===\n",
    "X_attack = np.vstack([feat_mem, feat_non])\n",
    "y_attack = np.concatenate([np.ones(len(feat_mem)), np.zeros(len(feat_non))])\n",
    "\n",
    "\n",
    "print(\"X attack is\",X_attack)\n",
    "print(\"y attack is\",y_attack)\n",
    "print(\"X attack shape:\", X_attack.shape)\n",
    "print(\"y attack shape:\", y_attack.shape)\n",
    "\n",
    "#stratify=y_attack argument in train_test_split ensures that the proportion of member\n",
    "#and non-member samples remains the same in both the training and test splits of the attack dataset.\n",
    "\n",
    "\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(\n",
    "    X_attack, y_attack, test_size=0.3, random_state=0, stratify=y_attack\n",
    ")\n",
    "\n",
    "print(\"Xa_train shape:\", Xa_train.shape)\n",
    "print(\"Xa_test shape:\", Xa_test.shape)\n",
    "print(\"ya_train shape:\", ya_train.shape)\n",
    "print(\"ya_test shape:\", ya_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df5bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MIA Success Rate (AUC): 49.12%\n",
      "✅ MIA Accuracy: 49.59%\n"
     ]
    }
   ],
   "source": [
    "# === Train Attack Model ===\n",
    "attack_model = LogisticRegression(solver=\"lbfgs\")\n",
    "attack_model.fit(Xa_train, ya_train)\n",
    "\n",
    "# === Evaluate Attack ===\n",
    "ya_proba = attack_model.predict_proba(Xa_test)[:, 1]\n",
    "ya_preds = attack_model.predict(Xa_test)\n",
    "\n",
    "auc = roc_auc_score(ya_test, ya_proba)\n",
    "acc = accuracy_score(ya_test, ya_preds)\n",
    "\n",
    "print(f\"✅ MIA Success Rate (AUC): {auc * 100:.2f}%\")\n",
    "print(f\"✅ MIA Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244943a",
   "metadata": {},
   "source": [
    "- Purpose:\n",
    "The code demonstrates a privacy attack where one investigates if the per-sample loss (MSE) reveals whether a sample was part of the training set.\n",
    "- Flow:\n",
    "It first calculates the losses for both membership (seen) and non-membership (unseen) datasets, creates a combined dataset from these loss values, trains a logistic regression model using these simple features, and finally evaluates the attack using accuracy and AUC metrics.\n",
    "- Implications:\n",
    "If the attack model achieves a high AUC or accuracy, it indicates that the target model might be leaking information about its training data. This is a key concern in the domain of machine learning privacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644545d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e117f629",
   "metadata": {},
   "source": [
    "MIA AUC: Tells you how well the attack distinguishes between member and non-member samples.\n",
    "\n",
    "MIA Accuracy: Fraction of correctly identified membership status (at default threshold of 0.5).\n",
    "\n",
    "\n",
    "\n",
    "The “MIA Success Rate” is simply how well your membership‐inference attack distinguishes “members” (records that were in the model’s training set) from “non-members” (records that were held out). In practice it’s reported in one of two ways:\n",
    "\n",
    "Attack Accuracy\n",
    "\n",
    "Accuracy=\n",
    "correctly labeled members\n",
    "+\n",
    "correctly labeled non-members\n",
    "total samples\n",
    ".\n",
    "Accuracy= \n",
    "total samples\n",
    "correctly labeled members+correctly labeled non-members\n",
    "​\n",
    " .\n",
    "If your attack simply guesses “in” or “out” for each sample (typically by thresholding a score at 0.5), the accuracy is the fraction of those guesses that are right.\n",
    "\n",
    "ROC-AUC (Area Under the Curve)\n",
    "Rather than pick a fixed threshold, you can plot the receiver-operating characteristic (ROC) curve (true-positive rate vs. false-positive rate as you sweep the threshold). The area under that curve (AUC) is a threshold-independent summary of attack effectiveness.\n",
    "\n",
    "AUC = 0.5 means “no better than random guessing.”\n",
    "\n",
    "AUC = 1.0 means a perfect attack.\n",
    "\n",
    "In MIA work even AUCs of 0.6–0.8 are considered substantial privacy leakage.\n",
    "\n",
    "You may also report other related figures:\n",
    "\n",
    "True-Positive Rate (TPR) at a fixed False-Positive Rate (FPR)—e.g. “TPR at FPR=10%”—to show, of the 10% of non-members you falsely flag as members, how many real members you catch.\n",
    "\n",
    "Precision / Recall / F1-score if you care about imbalanced member vs. non-member ratios.\n",
    "\n",
    "But in almost every MIA paper the headline “MIA Success Rate” is the attack’s ROC-AUC (or sometimes its plain accuracy) on a held-out test of member vs. non-member samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9da2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Attack:\n",
      " AUC        = 0.484\n",
      " Accuracy   = 0.488\n",
      " Precision  = 0.489\n",
      " Recall     = 0.545\n",
      " F1-score   = 0.516\n",
      " PR-AUC     = 0.489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "# Instantiate\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "\n",
    "# Train\n",
    "rf.fit(Xa_train, ya_train)\n",
    "\n",
    "# Predict\n",
    "probs = rf.predict_proba(Xa_test)[:,1]\n",
    "preds = rf.predict(Xa_test)\n",
    "\n",
    "# Metrics\n",
    "print(\"Random Forest Attack:\")\n",
    "print(f\" AUC        = {roc_auc_score or MIA Success Rate(ya_test, probs):.3f}\")\n",
    "print(f\" Accuracy   = {accuracy_score(ya_test, preds):.3f}\")\n",
    "print(f\" Precision  = {precision_score(ya_test, preds):.3f}\")\n",
    "print(f\" Recall     = {recall_score(ya_test, preds):.3f}\")\n",
    "print(f\" F1-score   = {f1_score(ya_test, preds):.3f}\")\n",
    "print(f\" PR-AUC     = {average_precision_score(ya_test, probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46db94e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (1.26.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/150.0 MB 4.8 MB/s eta 0:00:32\n",
      "    --------------------------------------- 2.6/150.0 MB 5.8 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 3.9/150.0 MB 5.7 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 5.8/150.0 MB 6.5 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 7.6/150.0 MB 7.0 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 9.4/150.0 MB 7.3 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 11.3/150.0 MB 7.7 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 13.1/150.0 MB 7.8 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 15.2/150.0 MB 8.0 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 17.6/150.0 MB 8.3 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 19.7/150.0 MB 8.5 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 21.5/150.0 MB 8.6 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 23.1/150.0 MB 8.7 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 26.5/150.0 MB 9.1 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 29.4/150.0 MB 9.4 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 31.2/150.0 MB 9.5 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 34.1/150.0 MB 9.7 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 35.9/150.0 MB 9.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 38.0/150.0 MB 9.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 40.1/150.0 MB 9.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 42.2/150.0 MB 9.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 44.6/150.0 MB 9.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 46.7/150.0 MB 9.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 48.5/150.0 MB 9.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 50.3/150.0 MB 9.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 52.2/150.0 MB 9.7 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 54.5/150.0 MB 9.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 56.4/150.0 MB 9.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 57.9/150.0 MB 9.6 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 59.8/150.0 MB 9.6 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 61.9/150.0 MB 9.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 64.0/150.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 67.6/150.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 69.7/150.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 71.8/150.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 73.7/150.0 MB 9.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 75.5/150.0 MB 9.6 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 76.8/150.0 MB 9.5 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 78.6/150.0 MB 9.5 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 80.2/150.0 MB 9.5 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 81.8/150.0 MB 9.4 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 9.4 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 84.9/150.0 MB 9.3 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 86.2/150.0 MB 9.3 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 87.6/150.0 MB 9.2 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 89.1/150.0 MB 9.2 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 90.4/150.0 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 91.8/150.0 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 93.3/150.0 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 94.9/150.0 MB 9.0 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 96.5/150.0 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 98.0/150.0 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 99.9/150.0 MB 9.0 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 101.7/150.0 MB 9.0 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 103.3/150.0 MB 9.0 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 104.6/150.0 MB 8.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 106.4/150.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 108.3/150.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 110.1/150.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 111.9/150.0 MB 8.9 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 114.8/150.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 117.2/150.0 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 119.5/150.0 MB 9.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 121.6/150.0 MB 9.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 124.3/150.0 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 126.6/150.0 MB 9.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 129.0/150.0 MB 9.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 131.1/150.0 MB 9.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 133.2/150.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 135.3/150.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 137.1/150.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 139.5/150.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 140.0/150.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 142.1/150.0 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 144.2/150.0 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 145.8/150.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  147.8/150.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.2/150.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 9.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "956c9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Attack:\n",
      " AUC or MIA Success Rate   = 0.485\n",
      " Acc    = 0.487\n",
      " PR-AUC = 0.487\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
    "\n",
    "# Prepare DMatrix\n",
    "dtrain = xgb.DMatrix(Xa_train, label=ya_train)\n",
    "dtest  = xgb.DMatrix(Xa_test,  label=ya_test)\n",
    "\n",
    "# Params\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 4,\n",
    "    \"eta\": 0.1,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "# Train\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict\n",
    "probs = bst.predict(dtest)\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "print(\"XGBoost Attack:\")\n",
    "print(f\" AUC or MIA Success Rate   = {roc_auc_score(ya_test, probs):.3f}\")\n",
    "print(f\" Acc    = {accuracy_score(ya_test, preds):.3f}\")\n",
    "print(f\" PR-AUC = {average_precision_score(ya_test, probs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Attack:\n",
      " AUC      = 0.507\n",
      " Accuracy = 0.508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Note: need probability=True to get predict_proba\n",
    "svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=0)\n",
    "\n",
    "svm.fit(Xa_train, ya_train)\n",
    "probs = svm.predict_proba(Xa_test)[:,1]\n",
    "preds = svm.predict(Xa_test)\n",
    "\n",
    "print(\"SVM Attack:\")\n",
    "print(f\" AUC   or MIA Success Rate     = {roc_auc_score(ya_test, probs):.3f}\")\n",
    "print(f\" Accuracy = {accuracy_score(ya_test, preds):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Attack:\n",
      " AUC      = 0.509\n",
      " Accuracy = 0.500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu', max_iter=200, random_state=0)\n",
    "mlp.fit(Xa_train, ya_train)\n",
    "\n",
    "probs = mlp.predict_proba(Xa_test)[:,1]\n",
    "preds = mlp.predict(Xa_test)\n",
    "\n",
    "print(\"MLP Attack:\")\n",
    "print(f\" AUC  or MIA Success Rate      = {roc_auc_score(ya_test, probs):.3f}\")\n",
    "print(f\" Accuracy = {accuracy_score(ya_test, preds):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fefa811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 10, 'n_estimators': 50}\n",
      "Best CV AUC: 0.49900756886928876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [3,5,10]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=0),\n",
    "                    param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\n",
    "grid.fit(X_attack, y_attack)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV AUC:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "537335fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble AUC: 0.4853718711482797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble = VotingClassifier([\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(32,)))\n",
    "], voting='soft')\n",
    "\n",
    "ensemble.fit(Xa_train, ya_train)\n",
    "probs = ensemble.predict_proba(Xa_test)[:,1]\n",
    "print(\"Ensemble AUC:\", roc_auc_score(ya_test, probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef552da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbf500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9adc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893d5bb5",
   "metadata": {},
   "source": [
    "# OTHER TYPES ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e8c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c966064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mem, X_non, y_mem, y_non, s_mem, s_non = train_test_split(\n",
    "    X, y, specs, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71797e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(Xa, Sa, ya, batch_size=256):\n",
    "    ds = RULCombinedDataset(Xa, Sa, ya)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Helper: TPR @ FPR\n",
    "def tpr_at_fpr(y_true, y_scores, fpr_target=0.1):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    idx = np.searchsorted(fpr, fpr_target, side=\"right\")\n",
    "    return tpr[idx] if idx < len(tpr) else tpr[-1]\n",
    "\n",
    "# 1. Threshold Attack (max confidence)\n",
    "# def threshold_attack():\n",
    "#     # Query softmax probabilities\n",
    "#     mem_probs, non_probs = [], []\n",
    "#     loader_mem = get_loader(X_mem, s_mem, y_mem)\n",
    "#     loader_non = get_loader(X_non, s_non, y_non)\n",
    "#     with torch.no_grad():\n",
    "#         for loader, store in [(loader_mem, mem_probs), (loader_non, non_probs)]:\n",
    "#             for xc, xt, yb in loader:\n",
    "#                 xc, xt = xc.to(DEVICE), xt.to(DEVICE)\n",
    "#                 logits = model(xc, xt)\n",
    "#                 # if your model returns a scalar regression, skip; this attack applies to classification\n",
    "#                 probs = torch.softmax(logits, dim=1)  # classification case\n",
    "#                 max_conf = probs.max(dim=1)[0].cpu().numpy()\n",
    "#                 store.append(max_conf)\n",
    "#     mem_conf = np.concatenate(mem_probs)\n",
    "#     non_conf = np.concatenate(non_probs)\n",
    "#     scores = np.concatenate([mem_conf, non_conf])\n",
    "#     labels = np.concatenate([np.ones_like(mem_conf), np.zeros_like(non_conf)])\n",
    "#     # Evaluate\n",
    "#     auc = roc_auc_score(labels, scores)\n",
    "#     # Choose threshold at equal error rate or 0.5\n",
    "#     thresh = 0.5\n",
    "#     preds = (scores >= thresh).astype(int)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     tpr = tpr_at_fpr(labels, scores, fpr_target=0.1)\n",
    "#     print(\"Threshold Attack:\")\n",
    "#     print(f\" AUC={auc:.3f}, Acc={acc:.3f}, TPR@FPR=0.1={tpr:.3f}\")\n",
    "\n",
    "# # 2. Neural Attack on full probability vector\n",
    "# def neural_prob_attack():\n",
    "#     # Collect full softmax vectors\n",
    "#     mem_vecs, non_vecs = [], []\n",
    "#     for loader, store in [(get_loader(X_mem, s_mem, y_mem), mem_vecs),\n",
    "#                           (get_loader(X_non, s_non, y_non), non_vecs)]:\n",
    "#         with torch.no_grad():\n",
    "#             for xc, xt, yb in loader:\n",
    "#                 xc, xt = xc.to(DEVICE), xt.to(DEVICE)\n",
    "#                 logits = model(xc, xt)\n",
    "#                 probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "#                 store.append(probs)\n",
    "#     Xp = np.vstack([np.concatenate(mem_vecs), np.concatenate(non_vecs)])\n",
    "#     yp = np.concatenate([np.ones(len(mem_vecs)*loader.batch_size), np.zeros(len(non_vecs)*loader.batch_size)])\n",
    "#     # Attack train/test split\n",
    "#     Xa_tr, Xa_te, ya_tr, ya_te = train_test_split(Xp, yp, test_size=0.3, random_state=0, stratify=yp)\n",
    "#     atk = MLPClassifier(hidden_layer_sizes=(50,), max_iter=200)\n",
    "#     atk.fit(Xa_tr, ya_tr)\n",
    "#     probs = atk.predict_proba(Xa_te)[:,1]\n",
    "#     preds = atk.predict(Xa_te)\n",
    "#     auc = roc_auc_score(ya_te, probs)\n",
    "#     acc = accuracy_score(ya_te, preds)\n",
    "#     tpr = tpr_at_fpr(ya_te, probs)\n",
    "#     print(\"Neural Prob Attack:\")\n",
    "#     print(f\" AUC={auc:.3f}, Acc={acc:.3f}, TPR@FPR=0.1={tpr:.3f}\")\n",
    "\n",
    "# 3. Representation Attack (extract cont embedding)\n",
    "def rep_attack():\n",
    "    mem_emb, non_emb = [], []\n",
    "    loader_mem = get_loader(X_mem, s_mem, y_mem)\n",
    "    loader_non = get_loader(X_non, s_non, y_non)\n",
    "    with torch.no_grad():\n",
    "        for loader, store in [(loader_mem, mem_emb), (loader_non, non_emb)]:\n",
    "            for xc, xt, yb in loader:\n",
    "                xc, xt = xc.to(DEVICE), xt.to(DEVICE)\n",
    "                emb = model.tf(xt)  # cont embedding\n",
    "                store.append(emb.cpu().numpy())\n",
    "    Xr = np.vstack([np.concatenate(mem_emb), np.concatenate(non_emb)])\n",
    "    yr = np.concatenate([np.ones(len(mem_emb)*loader.batch_size), np.zeros(len(non_emb)*loader.batch_size)])\n",
    "    Xr_tr, Xr_te, yr_tr, yr_te = train_test_split(Xr, yr, test_size=0.3, random_state=0, stratify=yr)\n",
    "    atk = LogisticRegression(max_iter=200)\n",
    "    atk.fit(Xr_tr, yr_tr)\n",
    "    probs = atk.predict_proba(Xr_te)[:,1]\n",
    "    preds = atk.predict(Xr_te)\n",
    "    auc = roc_auc_score(yr_te, probs)\n",
    "    acc = accuracy_score(yr_te, preds)\n",
    "    tpr = tpr_at_fpr(yr_te, probs)\n",
    "    print(\"Representation Attack:\")\n",
    "    print(f\" AUC={auc:.3f}, Acc={acc:.3f}, TPR@FPR=0.1={tpr:.3f}\")\n",
    "\n",
    "# 4. Gradient-Norm Attack (white-box)\n",
    "def grad_norm_attack():\n",
    "    mem_norms, non_norms = [], []\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    for loader, store in [(get_loader(X_mem, s_mem, y_mem), mem_norms),\n",
    "                          (get_loader(X_non, s_non, y_non), non_norms)]:\n",
    "        for xc, xt, yb in loader:\n",
    "            xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            preds = model(xc, xt)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            loss.backward()\n",
    "            # grad norm aggregated over parameters\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_norm += p.grad.detach().norm().item()**2\n",
    "            total_norm = total_norm**0.5\n",
    "            store.append(total_norm)\n",
    "    nm = np.array(mem_norms)\n",
    "    nnm = np.array(non_norms)\n",
    "    scores = np.concatenate([nm, nnm])\n",
    "    labels = np.concatenate([np.ones_like(nm), np.zeros_like(nnm)])\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    thresh = np.median(scores)\n",
    "    preds = (scores >= thresh).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    tpr = tpr_at_fpr(labels, scores)\n",
    "    print(\"Gradient-Norm Attack:\")\n",
    "    print(f\" AUC={auc:.3f}, Acc={acc:.3f}, TPR@FPR=0.1={tpr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Attack:\n",
      " AUC=0.500, Acc=0.500, TPR@FPR=0.1=1.000\n",
      "Gradient-Norm Attack:\n",
      " AUC=0.337, Acc=0.462, TPR@FPR=0.1=0.000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # threshold_attack()\n",
    "    # neural_prob_attack()\n",
    "    # rep_attack()\n",
    "    grad_norm_attack()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1edad",
   "metadata": {},
   "source": [
    "Threshold Attack (using max predicted probability)\n",
    "\n",
    "Neural Attack (MLP on full softmax vector)\n",
    "\n",
    "Representation Attack (logistic regression on the model’s time-series embedding)\n",
    "\n",
    "Gradient-Norm Attack (white-box, using per-sample gradient norms)\n",
    "\n",
    "Each attack prints AUC, Accuracy, and TPR at FPR=0.1 so you can directly compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f7788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd0bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9684e4d4",
   "metadata": {},
   "source": [
    "# ADVANCED MIA ATTACK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070a4e",
   "metadata": {},
   "source": [
    "black-box (loss), gray-box (embedding), and white-box (gradient) features along with time-series-specific seasonality/trend features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728eb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "# === Paths ===\n",
    "H5_PATH         = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "ENCODER_PATH    = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\spec_encoder.joblib\"\n",
    "CHECKPOINT_PATH = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\\CombinedRULModel-DP-20250618_160955\\checkpoint.pth\"\n",
    "DEVICE          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load windows dataset ===\n",
    "with h5py.File(H5_PATH, \"r\") as f:\n",
    "    X = f[\"X_windows\"][:]\n",
    "    y = f[\"y_labels\"][:]\n",
    "    specs = f[\"specs_per_window\"][:]\n",
    "\n",
    "# === Dataset & Model definitions (as before) ===\n",
    "class RULCombinedDataset(Dataset):\n",
    "    def __init__(self, X, specs, y):\n",
    "        self.X = X; self.specs = specs; self.y = y.reshape(-1,1)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.from_numpy(self.specs[i]).long(),\n",
    "            torch.from_numpy(self.X[i]).float(),\n",
    "            torch.from_numpy(self.y[i]).float()\n",
    "        )\n",
    "\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        return x[:, -1, :]\n",
    "\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "        super().__init__()\n",
    "        self.tf = TimeSeriesEmbedder(num_sensor_features, continuous_dim)\n",
    "        if cont_mean_std is None:\n",
    "            cont_mean_std = torch.stack([torch.zeros(continuous_dim), torch.ones(continuous_dim)], dim=1)\n",
    "        self.tab = TabTransformer(\n",
    "            categories=categories,\n",
    "            num_continuous=continuous_dim,\n",
    "            dim=continuous_dim,\n",
    "            dim_out=1,\n",
    "            depth=6, heads=8,\n",
    "            attn_dropout=0.1, ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4,2), mlp_act=nn.ReLU(),\n",
    "            continuous_mean_std=cont_mean_std\n",
    "        )\n",
    "    def forward(self, x_cat, x_ts):\n",
    "        cont = self.tf(x_ts)\n",
    "        return self.tab(x_cat, cont)\n",
    "\n",
    "# === Split members/non-members ===\n",
    "X_mem, X_non, y_mem, y_non, s_mem, s_non = train_test_split(\n",
    "    X, y, specs, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# === Load model ===\n",
    "encoder = joblib.load(ENCODER_PATH)\n",
    "cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "model = CombinedRULModel(\n",
    "    num_sensor_features=X.shape[2],\n",
    "    context_length=X.shape[1],\n",
    "    categories=cat_sizes,\n",
    "    continuous_dim=128\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# === Helpers ===\n",
    "def get_loader(Xa, Sa, ya, bs=256):\n",
    "    return DataLoader(RULCombinedDataset(Xa, Sa, ya), batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# given true labels and attack‐scores, returns the True-Positive Rate when the \n",
    "# False-Positive Rate is at most fpr_target (e.g. 10%).\n",
    "def tpr_at_fpr(y_true, y_scores, fpr_target=0.1):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    idx = np.searchsorted(fpr, fpr_target, side=\"right\")\n",
    "    return tpr[idx] if idx < len(tpr) else tpr[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Feature Extractors ===\n",
    "\n",
    "# Loss (black-box)\n",
    "# xc: categorical specs; xt: time-series window; yb: true RUL.\n",
    "# Computes per-sample MSE as a 1D feature.\n",
    "# 1) Loss (MSE)\n",
    "def feat_loss(xc, xt, yb):\n",
    "    with torch.no_grad():\n",
    "        preds = model(xc, xt)\n",
    "    return ((preds - yb)**2).cpu().numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "# Embedding Norm (gray-box)\n",
    "# Runs only the TimeSeriesEmbedder on the window.Takes the ℓ₂‐norm of each \n",
    "# embedding vector (detached so that gradients aren’t tracked).\n",
    "# 2) Embedding norm (detached)\n",
    "def feat_emb_norm(xt):\n",
    "    emb = model.tf(xt)\n",
    "    return torch.norm(emb.detach(), dim=1).cpu().numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "#  Gradient Norm (white-box)\n",
    "# Computes the gradient of the loss w.r.t. all parameters, then aggregates their \n",
    "# norms.Broadcasts the same gradient‐norm scalar across the batch as a feature.\n",
    "\n",
    "# 3) Gradient norm (white-box)\n",
    "# 3) Gradient norm (white‐box), fixed to use .item()\n",
    "def feat_grad_norm(xc, xt, yb):\n",
    "    model.zero_grad()\n",
    "    preds = model(xc, xt)\n",
    "    loss = torch.nn.functional.mse_loss(preds, yb, reduction=\"sum\")\n",
    "    loss.backward()\n",
    "    total_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            # use .item() to get Python float\n",
    "            norm = p.grad.detach().norm().item()\n",
    "            total_sq += norm**2\n",
    "    total = np.sqrt(total_sq)\n",
    "    # return shape (batch,1): we broadcast the same value across the batch\n",
    "    return np.full((xc.size(0), 1), total)\n",
    "\n",
    "\n",
    "# Seasonality: takes the first 5 Fourier magnitudes for each series in the window.\n",
    "# Trend: fits a 3rd‐degree polynomial to each feature’s time‐series via least \n",
    "# squares; concatenates all coefficient vectors.\n",
    "\n",
    "# 4) Trend & Seasonality\n",
    "def feat_trend_season(xt_np):\n",
    "    B, T, M = xt_np.shape\n",
    "    Xf = np.fft.rfft(xt_np, axis=1)\n",
    "    season = np.abs(Xf[:, :5, :]).reshape(B, -1)\n",
    "    t = np.linspace(0,1,T)\n",
    "    V = np.vander(t, N=4, increasing=True)\n",
    "    coeffs = []\n",
    "    for b in range(B):\n",
    "        cs = []\n",
    "        for m in range(M):\n",
    "            c, *_ = np.linalg.lstsq(V, xt_np[b,:,m], rcond=None)\n",
    "            cs.append(c)\n",
    "        coeffs.append(np.concatenate(cs))\n",
    "    trend = np.stack(coeffs, axis=0)\n",
    "    return np.hstack([season, trend])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loops over both member and non-member splits.\n",
    "# Extracts all four features for each batch.\n",
    "# Stacks them horizontally into a single feature vector per sample.\n",
    "# Labels members as 1, non-members as 0.\n",
    "# === Collect features + labels ===\n",
    "def collect_features(Xa, Sa, ya, label):\n",
    "    loader = get_loader(Xa, Sa, ya)\n",
    "    feats, labs = [], []\n",
    "    for xc, xt, yb in loader:\n",
    "        xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "        xt_np = xt.cpu().numpy()\n",
    "        L  = feat_loss(xc, xt, yb)\n",
    "        E  = feat_emb_norm(xt)\n",
    "        G  = feat_grad_norm(xc, xt, yb)\n",
    "        TS = feat_trend_season(xt_np)\n",
    "        batch_f = np.hstack([L, E, G, TS])\n",
    "        feats.append(batch_f)\n",
    "        labs.extend([label]*batch_f.shape[0])\n",
    "    return np.vstack(feats), np.array(labs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df562a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fm, ym = collect_features(X_mem, s_mem, y_mem, 1)\n",
    "Fn, yn = collect_features(X_non, s_non, y_non, 0)\n",
    "\n",
    "X_attack = np.vstack([Fm, Fn])\n",
    "y_attack = np.concatenate([ym, yn])\n",
    "\n",
    "# === Split & train attack ===\n",
    "Xa_tr, Xa_te, ya_tr, ya_te = train_test_split(\n",
    "    X_attack, y_attack, test_size=0.3,\n",
    "    random_state=0, stratify=y_attack\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1328ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extended MIA Results ===\n",
      "AUC         : 0.5742\n",
      "Accuracy    : 0.5331\n",
      "TPR@FPR=0.1 : 0.1211\n",
      "Precision   : 0.5378\n",
      "Recall      : 0.4700\n",
      "F1-score    : 0.5016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "atk = LogisticRegression(max_iter=500)\n",
    "atk.fit(Xa_tr, ya_tr)\n",
    "\n",
    "probs = atk.predict_proba(Xa_te)[:,1]\n",
    "preds = atk.predict(Xa_te)\n",
    "\n",
    "# === Metrics ===\n",
    "auc  = roc_auc_score(ya_te, probs)\n",
    "acc  = accuracy_score(ya_te, preds)\n",
    "tpr1 = tpr_at_fpr(ya_te, probs, fpr_target=0.1)\n",
    "prec = precision_score(ya_te, preds)\n",
    "rec  = recall_score(ya_te, preds)\n",
    "f1   = f1_score(ya_te, preds)\n",
    "\n",
    "print(\"=== Extended MIA Results ===\")\n",
    "print(f\"AUC         : {auc:.4f}\")\n",
    "print(f\"Accuracy    : {acc:.4f}\")\n",
    "print(f\"TPR@FPR=0.1 : {tpr1:.4f}\")\n",
    "print(f\"Precision   : {prec:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263b737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
