{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e6f04b",
   "metadata": {},
   "source": [
    "# TRY1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850a7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_rul_pipeline.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import h5py\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# === Hard-coded paths ===\n",
    "WINDOW_CSV     = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\super_same_norm.csv\"\n",
    "SPEC_CSV       = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\data\\train_specifications.csv\"\n",
    "ENCODER_PATH   = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\spec_encoder.joblib\"\n",
    "H5_PATH        = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "ARTIFACT_ROOT  = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\artifacts\"\n",
    "VALIDATION_CSV = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\validation_super_same_norm.csv\"\n",
    "VALIDATION_H5  = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\validation_data.h5\"\n",
    "\n",
    "SENSOR_FEATURES = [\n",
    "    '171_0', '666_0', '427_0', '837_0', '167_0', '167_1', '167_2', '167_3', '167_4',\n",
    "    '167_5', '167_6', '167_7', '167_8', '167_9', '309_0', '272_0', '272_1', '272_2',\n",
    "    '272_3', '272_4', '272_5', '272_6', '272_7', '272_8', '272_9', '835_0', '370_0',\n",
    "    '291_0', '291_1', '291_2', '291_3', '291_4', '291_5', '291_6', '291_7', '291_8',\n",
    "    '291_9', '291_10', '158_0', '158_1', '158_2', '158_3', '158_4', '158_5', '158_6',\n",
    "    '158_7', '158_8', '158_9', '100_0', '459_0', '459_1', '459_2', '459_3', '459_4',\n",
    "    '459_5', '459_6', '459_7', '459_8', '459_9', '459_10', '459_11', '459_12', '459_13',\n",
    "    '459_14', '459_15', '459_16', '459_17', '459_18', '459_19', '397_0', '397_1', '397_2',\n",
    "    '397_3', '397_4', '397_5', '397_6', '397_7', '397_8', '397_9', '397_10', '397_11',\n",
    "    '397_12', '397_13', '397_14', '397_15', '397_16', '397_17', '397_18', '397_19',\n",
    "    '397_20', '397_21', '397_22', '397_23', '397_24', '397_25', '397_26', '397_27',\n",
    "    '397_28', '397_29', '397_30', '397_31', '397_32', '397_33', '397_34', '397_35'\n",
    "]\n",
    "\n",
    "# === Utils ===\n",
    "def create_X_y(csv_path=WINDOW_CSV, sensor_features=None, context=70, verbose=True):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X,y,vids = [],[],[]\n",
    "    for vid,grp in df.groupby(\"vehicle_id\"):\n",
    "        data = grp[sensor_features].values\n",
    "        rul  = grp[\"RUL\"].values\n",
    "        if len(data)<context:\n",
    "            if verbose: print(f\"Skipping {vid}, len<{context}\")\n",
    "            continue\n",
    "        for i in range(len(data)-context+1):\n",
    "            X.append(data[i:i+context])\n",
    "            y.append(rul[i+context-1])\n",
    "            vids.append(vid)\n",
    "    X=np.stack(X); y=np.array(y); vids=np.array(vids)\n",
    "    if verbose: print(f\"Windows: {len(X)}, shape={X.shape[1:]}\")\n",
    "    spec_df = pd.read_csv(SPEC_CSV)\n",
    "    spec_cols=[f\"Spec_{i}\" for i in range(8)]\n",
    "    enc=OrdinalEncoder()\n",
    "    spec_df[spec_cols]=enc.fit_transform(spec_df[spec_cols])\n",
    "    specs = (\n",
    "        pd.DataFrame({\"vehicle_id\":vids})\n",
    "          .merge(spec_df[[\"vehicle_id\"]+spec_cols],on=\"vehicle_id\")\n",
    "    )[spec_cols].values.astype(int)\n",
    "    joblib.dump(enc, ENCODER_PATH)\n",
    "    if verbose: print(f\"Saved encoder→{ENCODER_PATH}\")\n",
    "    return X,y,vids,specs\n",
    "\n",
    "def save_to_h5(X,y,vids,specs,h5_path=H5_PATH):\n",
    "    with h5py.File(h5_path,\"w\") as f:\n",
    "        f.create_dataset(\"X_windows\",data=X,compression=\"gzip\")\n",
    "        f.create_dataset(\"y_labels\", data=y,compression=\"gzip\")\n",
    "        f.create_dataset(\"window_vids\",data=vids,compression=\"gzip\")\n",
    "        f.create_dataset(\"specs_per_window\",data=specs,compression=\"gzip\")\n",
    "    print(f\"Saved H5→{h5_path}\")\n",
    "\n",
    "def load_from_h5(h5_path=H5_PATH):\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        X=f[\"X_windows\"][:]\n",
    "        y=f[\"y_labels\"][:]\n",
    "        vids=f[\"window_vids\"][:]\n",
    "        specs=f[\"specs_per_window\"][:]\n",
    "    return X,y,vids,specs\n",
    "\n",
    "class RULCombinedDataset(Dataset):\n",
    "    def __init__(self,X,specs,y):\n",
    "        self.X=X; self.specs=specs; self.y=y.reshape(-1,1)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self,i):\n",
    "        return (\n",
    "            torch.from_numpy(self.specs[i]).long(),\n",
    "            torch.from_numpy(self.X[i]).float(),\n",
    "            torch.from_numpy(self.y[i]).float()\n",
    "        )\n",
    "\n",
    "def make_artifact_folder(model_name,pvt):\n",
    "    ts=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    suffix=\"DP\" if pvt else \"NDP\"\n",
    "    folder=f\"{model_name}-{suffix}-{ts}\"\n",
    "    path=os.path.join(ARTIFACT_ROOT,folder)\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# === Models ===\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    def __init__(self,num_features,d_model=128,n_heads=8,num_layers=2,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj=nn.Linear(num_features,d_model)\n",
    "        enc_layer=nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,nhead=n_heads,dropout=dropout,batch_first=True\n",
    "        )\n",
    "        self.encoder=nn.TransformerEncoder(enc_layer,num_layers=num_layers)\n",
    "    def forward(self,x):\n",
    "        x=self.input_proj(x)\n",
    "        x=self.encoder(x)\n",
    "        return x[:,-1,:]\n",
    "\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self,num_sensor_features,context_length,categories,continuous_dim,cont_mean_std=None):\n",
    "        super().__init__()\n",
    "        self.tf=TimeSeriesEmbedder(num_sensor_features,continuous_dim)\n",
    "        if cont_mean_std is None:\n",
    "            cont_mean_std=torch.stack([torch.zeros(continuous_dim),torch.ones(continuous_dim)],dim=1)\n",
    "        self.tab=TabTransformer(\n",
    "            categories=categories,\n",
    "            num_continuous=continuous_dim,\n",
    "            dim=continuous_dim,\n",
    "            dim_out=1,\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4,2),\n",
    "            mlp_act=nn.ReLU(),\n",
    "            continuous_mean_std=cont_mean_std\n",
    "        )\n",
    "    def forward(self,x_cat,x_ts):\n",
    "        cont=self.tf(x_ts)\n",
    "        return self.tab(x_cat,cont)\n",
    "\n",
    "# === Services ===\n",
    "def get_criterion(): return MSELoss()\n",
    "def get_optimizer(model,lr=1e-3): return Adam(model.parameters(),lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98be2df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_LinalgBackend.Magma: 2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cuda.preferred_linalg_library(\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06465852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spectral_dp_batch(model, criterion, optimizer, x_cat, x_ts, yb,\n",
    "                            clipping_norm, noise_scale, delta):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_cat, x_ts)\n",
    "    tloss = criterion(output, yb)\n",
    "    tloss.backward()\n",
    "\n",
    "    grads = [p.grad.detach() for p in model.parameters() if p.grad is not None]\n",
    "\n",
    "    valid_grads = []\n",
    "    for g in grads:\n",
    "        if torch.isnan(g).any() or torch.isinf(g).any():\n",
    "            print(\"Invalid gradient tensor encountered. Skipping batch.\")\n",
    "            return\n",
    "        valid_grads.append(g)\n",
    "\n",
    "    # Flatten and stack\n",
    "    grads_flat = torch.cat([g.reshape(-1) for g in valid_grads])\n",
    "    num_params = grads_flat.shape[0]\n",
    "    G = grads_flat.reshape(num_params, 1)\n",
    "\n",
    "    if torch.isnan(G).any() or torch.isinf(G).any():\n",
    "        print(\"NaNs/Infs in flattened gradient matrix G. Skipping batch.\")\n",
    "        return\n",
    "\n",
    "    # Try GPU SVD, fallback to CPU if fails\n",
    "    try:\n",
    "        U, S, Vh = torch.linalg.svd(G, full_matrices=False)\n",
    "    except RuntimeError as e:\n",
    "        print(\"SVD failed on GPU, trying CPU fallback. Error:\", e)\n",
    "        try:\n",
    "            G_cpu = G.cpu()\n",
    "            U_cpu, S_cpu, Vh_cpu = torch.linalg.svd(G_cpu, full_matrices=False)\n",
    "            # Move back to GPU\n",
    "            U = U_cpu.to(G.device)\n",
    "            S = S_cpu.to(G.device)\n",
    "            Vh = Vh_cpu.to(G.device)\n",
    "        except RuntimeError as e2:\n",
    "            print(\"SVD also failed on CPU. Skipping batch. Error:\", e2)\n",
    "            return\n",
    "\n",
    "    # Clip and noised singular values\n",
    "    S_clipped = torch.clamp(S, max=clipping_norm)\n",
    "    noise = torch.normal(mean=0, std=noise_scale, size=S.shape, device=S.device)\n",
    "    S_noisy = S_clipped + noise\n",
    "\n",
    "    G_noisy = (U @ torch.diag(S_noisy) @ Vh)\n",
    "\n",
    "    # Unflatten and write back noisy gradients\n",
    "    offset = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        numel = p.grad.numel()\n",
    "        p.grad.data = G_noisy[offset:offset+numel].reshape(p.grad.shape)\n",
    "        offset += numel\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9d4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.backends.cuda.preferred_linalg_library(\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1886066",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# I AM STARTING HERE :)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_h5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpvt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(use_h5, pvt)\u001b[0m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pvt:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtrain_spectral_dp_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mDP_CLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDP_NOISE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDP_DELTA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(model(x_cat, x_ts), yb)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mtrain_spectral_dp_batch\u001b[1;34m(model, criterion, optimizer, x_cat, x_ts, yb, clipping_norm, noise_scale, delta)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Try GPU SVD, fallback to CPU if fails\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     U, S, Vh \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD failed on GPU, trying CPU fallback. Error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(use_h5=True, pvt=False):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE, NUM_EPOCHS, LR = 1, 5, 1e-3\n",
    "    LR_PAT, ES_PAT, LR_F = 5, 11, 0.5\n",
    "\n",
    "    # DP params\n",
    "    DP_CLIP, DP_NOISE, DP_DELTA = 1.0, 1.0, 1e-5\n",
    "\n",
    "    # data load\n",
    "    if use_h5 and os.path.exists(H5_PATH):\n",
    "        X, y, _, specs = load_from_h5()\n",
    "    else:\n",
    "        X, y, _, specs = create_X_y(sensor_features=SENSOR_FEATURES)\n",
    "        save_to_h5(X, y, _, specs)\n",
    "\n",
    "    encoder = joblib.load(ENCODER_PATH)\n",
    "    cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "    Xtr, Xv, str_, sv, yr, yv = train_test_split(X, specs, y, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(RULCombinedDataset(Xtr, str_, yr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(RULCombinedDataset(Xv, sv, yv), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # setup artifacts, metadata\n",
    "    art = make_artifact_folder(\"CombinedRULModel\",pvt)\n",
    "    logp, metap, ckpt = [os.path.join(art, fn) for fn in (\"train_val_log.txt\",\"metadata.json\",\"checkpoint.pth\")]\n",
    "    meta = {\"model_name\":\"CombinedRULModel\",\"num_sensor_features\":X.shape[2],\n",
    "            \"context_length\":X.shape[1],\"continuous_dim\":128,\n",
    "            \"categories\":list(cat_sizes),\"batch_size\":BATCH_SIZE,\n",
    "            \"learning_rate\":LR,\"num_epochs\":NUM_EPOCHS,\"pvt\":pvt}\n",
    "    with open(metap,\"w\") as f: json.dump(meta,f,indent=4)\n",
    "\n",
    "    model = CombinedRULModel(X.shape[2],X.shape[1],cat_sizes,128).to(DEVICE)\n",
    "    criterion, optimizer = MSELoss(), Adam(model.parameters(),lr=LR)\n",
    "    scheduler = StepLR(optimizer,step_size=1,gamma=LR_F)\n",
    "\n",
    "    best, noimp = float('inf'), 0\n",
    "    with open(logp,\"w\") as f: f.write(\"epoch,train_loss,val_loss,epoch_time,lr,notes\\n\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for ep in range(1, NUM_EPOCHS+1):\n",
    "        ep_start = time.perf_counter()\n",
    "        lr_cur = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # training\n",
    "        model.train(); train_loss=0\n",
    "        for x_cat, x_ts, yb in train_loader:\n",
    "            x_cat, x_ts, yb = x_cat.to(DEVICE), x_ts.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            if pvt:\n",
    "                train_spectral_dp_batch(model, criterion, optimizer, x_cat, x_ts, yb,\n",
    "                                        DP_CLIP, DP_NOISE, DP_DELTA)\n",
    "            else:\n",
    "                loss = criterion(model(x_cat, x_ts), yb)\n",
    "                loss.backward(); optimizer.step(); train_loss += loss.item()*yb.size(0)\n",
    "        if not pvt:\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval(); val_loss=0\n",
    "        with torch.no_grad():\n",
    "            for x_cat, x_ts, yb in val_loader:\n",
    "                x_cat, x_ts, yb = x_cat.to(DEVICE), x_ts.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = criterion(model(x_cat, x_ts), yb)\n",
    "                val_loss += loss.item()*yb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # logging & sched\n",
    "        elapsed = time.perf_counter()-ep_start\n",
    "        h,m,s = map(int,[elapsed//3600,(elapsed%3600)//60,elapsed%60])\n",
    "        notes=\"\"\n",
    "        if val_loss<best:\n",
    "            best,val_notes=val_loss,f\"Model saved at epoch {ep}\"\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            notes=val_notes\n",
    "        else:\n",
    "            noimp+=1\n",
    "            if noimp%LR_PAT==0: scheduler.step(); notes+=\" LR stepped\"\n",
    "            if noimp>=ES_PAT: notes+=\" Early stopping\"\n",
    "\n",
    "        with open(logp,\"a\") as f:\n",
    "            f.write(f\"{ep},{train_loss:.6f},{val_loss:.6f},{h:02d}:{m:02d}:{s:02d},{lr_cur:.6g},{notes}\\n\")\n",
    "        print(f\"Epoch {ep:02d} Train {train_loss:.4f} Val {val_loss:.4f} Time {h:02d}:{m:02d}:{s:02d} LR {lr_cur:.2e} {notes}\")\n",
    "        if noimp>=ES_PAT: break\n",
    "\n",
    "    total = time.perf_counter()-start\n",
    "    h,m,s = map(int,[total//3600,(total%3600)//60,total%60]); tt=f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "    with open(metap,\"r+\") as f:\n",
    "        d=json.load(f); d[\"total_training_time\"]=tt; f.seek(0); json.dump(d,f,indent=4); f.truncate()\n",
    "    print(f\"Done. Best val MSE {best:.4f}. Total time {tt}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I AM STARTING HERE :)\n",
    "if __name__==\"__main__\":\n",
    "   \n",
    "    train(use_h5=True, pvt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b52c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!taskkill /PID 9412 /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88783f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[4], line 98\n",
    "     95 # I AM STARTING HERE :)\n",
    "     96 if __name__==\"__main__\":\n",
    "---> 98     train(use_h5=True, pvt=True)\n",
    "\n",
    "Cell In[4], line 47\n",
    "     45 model.train(); train_loss=0\n",
    "     46 for x_cat, x_ts, yb in train_loader:\n",
    "---> 47     x_cat, x_ts, yb = x_cat.to(DEVICE), x_ts.to(DEVICE), yb.to(DEVICE)\n",
    "     48     optimizer.zero_grad()\n",
    "     49     if pvt:\n",
    "\n",
    "RuntimeError: CUDA error: out of memory\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
    "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349676d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6fcc12d",
   "metadata": {},
   "source": [
    "# TRY2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82bbf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import json\n",
    "# import time\n",
    "# import h5py\n",
    "# import joblib\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from datetime import datetime\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tab_transformer_pytorch import TabTransformer\n",
    "# from torch.optim import Adam\n",
    "# from torch.nn import MSELoss\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# # === Hard-coded paths ===\n",
    "# WINDOW_CSV     = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\super_same_norm.csv\"\n",
    "# SPEC_CSV       = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\data\\train_specifications.csv\"\n",
    "# ENCODER_PATH   = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\spec_encoder.joblib\"\n",
    "# H5_PATH        = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "# ARTIFACT_ROOT  = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\artifacts\"\n",
    "# VALIDATION_CSV = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\validation_super_same_norm.csv\"\n",
    "# VALIDATION_H5  = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\validation_data.h5\"\n",
    "\n",
    "# SENSOR_FEATURES = [\n",
    "#     '171_0', '666_0', '427_0', '837_0', '167_0', '167_1', '167_2', '167_3', '167_4',\n",
    "#     '167_5', '167_6', '167_7', '167_8', '167_9', '309_0', '272_0', '272_1', '272_2',\n",
    "#     '272_3', '272_4', '272_5', '272_6', '272_7', '272_8', '272_9', '835_0', '370_0',\n",
    "#     '291_0', '291_1', '291_2', '291_3', '291_4', '291_5', '291_6', '291_7', '291_8',\n",
    "#     '291_9', '291_10', '158_0', '158_1', '158_2', '158_3', '158_4', '158_5', '158_6',\n",
    "#     '158_7', '158_8', '158_9', '100_0', '459_0', '459_1', '459_2', '459_3', '459_4',\n",
    "#     '459_5', '459_6', '459_7', '459_8', '459_9', '459_10', '459_11', '459_12', '459_13',\n",
    "#     '459_14', '459_15', '459_16', '459_17', '459_18', '459_19', '397_0', '397_1', '397_2',\n",
    "#     '397_3', '397_4', '397_5', '397_6', '397_7', '397_8', '397_9', '397_10', '397_11',\n",
    "#     '397_12', '397_13', '397_14', '397_15', '397_16', '397_17', '397_18', '397_19',\n",
    "#     '397_20', '397_21', '397_22', '397_23', '397_24', '397_25', '397_26', '397_27',\n",
    "#     '397_28', '397_29', '397_30', '397_31', '397_32', '397_33', '397_34', '397_35'\n",
    "# ]\n",
    "\n",
    "# # === Utils ===\n",
    "# def create_X_y(csv_path=WINDOW_CSV, sensor_features=SENSOR_FEATURES, context=70, verbose=True):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     X, y, vids = [], [], []\n",
    "#     for vid, grp in df.groupby(\"vehicle_id\"):\n",
    "#         data = grp[sensor_features].values\n",
    "#         rul = grp[\"RUL\"].values\n",
    "#         if len(data) < context:\n",
    "#             if verbose:\n",
    "#                 print(f\"Skipping {vid}, len<{context}\")\n",
    "#             continue\n",
    "#         for i in range(len(data) - context + 1):\n",
    "#             X.append(data[i:i+context])\n",
    "#             y.append(rul[i+context-1])\n",
    "#             vids.append(vid)\n",
    "#     X = np.stack(X)\n",
    "#     y = np.array(y)\n",
    "#     vids = np.array(vids)\n",
    "#     if verbose:\n",
    "#         print(f\"Windows: {len(X)}, shape={X.shape[1:]}\")\n",
    "\n",
    "#     spec_df = pd.read_csv(SPEC_CSV)\n",
    "#     spec_cols = [f\"Spec_{i}\" for i in range(8)]\n",
    "#     enc = OrdinalEncoder()\n",
    "#     spec_df[spec_cols] = enc.fit_transform(spec_df[spec_cols])\n",
    "#     specs = (\n",
    "#         pd.DataFrame({\"vehicle_id\": vids})\n",
    "#           .merge(spec_df[[\"vehicle_id\"] + spec_cols], on=\"vehicle_id\")\n",
    "#     )[spec_cols].values.astype(int)\n",
    "\n",
    "#     joblib.dump(enc, ENCODER_PATH)\n",
    "#     if verbose:\n",
    "#         print(f\"Saved encoder → {ENCODER_PATH}\")\n",
    "\n",
    "#     return X, y, vids, specs\n",
    "\n",
    "\n",
    "# def save_to_h5(X, y, vids, specs, h5_path=H5_PATH):\n",
    "#     with h5py.File(h5_path, \"w\") as f:\n",
    "#         f.create_dataset(\"X_windows\", data=X, compression=\"gzip\")\n",
    "#         f.create_dataset(\"y_labels\", data=y, compression=\"gzip\")\n",
    "#         f.create_dataset(\"window_vids\", data=vids, compression=\"gzip\")\n",
    "#         f.create_dataset(\"specs_per_window\", data=specs, compression=\"gzip\")\n",
    "#     print(f\"Saved H5 → {h5_path}\")\n",
    "\n",
    "\n",
    "# def load_from_h5(h5_path=H5_PATH):\n",
    "#     with h5py.File(h5_path, \"r\") as f:\n",
    "#         X = f[\"X_windows\"][:]\n",
    "#         y = f[\"y_labels\"][:]\n",
    "#         vids = f[\"window_vids\"][:]\n",
    "#         specs = f[\"specs_per_window\"][:]\n",
    "#     return X, y, vids, specs\n",
    "\n",
    "\n",
    "# class RULCombinedDataset(Dataset):\n",
    "#     def __init__(self, X, specs, y):\n",
    "#         self.X = X\n",
    "#         self.specs = specs\n",
    "#         self.y = y.reshape(-1, 1)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         return (\n",
    "#             torch.from_numpy(self.specs[i]).long(),\n",
    "#             torch.from_numpy(self.X[i]).float(),\n",
    "#             torch.from_numpy(self.y[i]).float()\n",
    "#         )\n",
    "\n",
    "\n",
    "# def make_artifact_folder(model_name):\n",
    "#     ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     suffix = \"NDP\"\n",
    "#     folder = f\"{model_name}-{suffix}-{ts}\"\n",
    "#     path = os.path.join(ARTIFACT_ROOT, folder)\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "#     return path\n",
    "\n",
    "# # === Models ===\n",
    "# class TimeSeriesEmbedder(nn.Module):\n",
    "#     def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.input_proj = nn.Linear(num_features, d_model)\n",
    "#         enc_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True\n",
    "#         )\n",
    "#         self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.input_proj(x)\n",
    "#         x = self.encoder(x)\n",
    "#         return x[:, -1, :]\n",
    "\n",
    "\n",
    "# class CombinedRULModel(nn.Module):\n",
    "#     def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "#         super().__init__()\n",
    "#         self.tf = TimeSeriesEmbedder(num_sensor_features, continuous_dim)\n",
    "#         if cont_mean_std is None:\n",
    "#             cont_mean_std = torch.stack([torch.zeros(continuous_dim), torch.ones(continuous_dim)], dim=1)\n",
    "#         self.tab = TabTransformer(\n",
    "#             categories=categories,\n",
    "#             num_continuous=continuous_dim,\n",
    "#             dim=continuous_dim,\n",
    "#             dim_out=1,\n",
    "#             depth=6,\n",
    "#             heads=8,\n",
    "#             attn_dropout=0.1,\n",
    "#             ff_dropout=0.1,\n",
    "#             mlp_hidden_mults=(4,2),\n",
    "#             mlp_act=nn.ReLU(),\n",
    "#             continuous_mean_std=cont_mean_std\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_cat, x_ts):\n",
    "#         cont = self.tf(x_ts)\n",
    "#         return self.tab(x_cat, cont)\n",
    "\n",
    "\n",
    "# # === Services ===\n",
    "# def get_criterion():\n",
    "#     return MSELoss()\n",
    "\n",
    "\n",
    "# def get_optimizer(model, lr=1e-3):\n",
    "#     return Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# # === Training function ===\n",
    "# def train(use_h5=False):\n",
    "#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     BATCH_SIZE = 256\n",
    "#     NUM_EPOCHS = 50\n",
    "#     LR = 1e-3\n",
    "#     LR_PAT = 5\n",
    "#     ES_PAT = 11\n",
    "#     LR_F = 0.5\n",
    "\n",
    "#     # data\n",
    "#     if use_h5 and os.path.exists(H5_PATH):\n",
    "#         X, y, _, specs = load_from_h5()\n",
    "#     else:\n",
    "#         X, y, _, specs = create_X_y(sensor_features=SENSOR_FEATURES)\n",
    "#         save_to_h5(X, y, _, specs)\n",
    "\n",
    "#     encoder = joblib.load(ENCODER_PATH)\n",
    "#     cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "#     Xtr, Xv, str_, sv, yr, yv = train_test_split(X, specs, y, test_size=0.2, random_state=42)\n",
    "#     tl = DataLoader(RULCombinedDataset(Xtr, str_, yr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     vl = DataLoader(RULCombinedDataset(Xv, sv, yv), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#     # artifacts\n",
    "#     art = make_artifact_folder(\"CombinedRULModel\")\n",
    "#     logp = os.path.join(art, \"train_val_log.txt\")\n",
    "#     metap = os.path.join(art, \"metadata.json\")\n",
    "#     ckpt = os.path.join(art, \"checkpoint.pth\")\n",
    "\n",
    "#     # metadata init\n",
    "#     meta = {\n",
    "#         \"model_name\": \"CombinedRULModel\",\n",
    "#         \"num_sensor_features\": X.shape[2],\n",
    "#         \"context_length\": X.shape[1],\n",
    "#         \"continuous_dim\": 128,\n",
    "#         \"categories\": list(cat_sizes),\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"learning_rate\": LR,\n",
    "#         \"num_epochs\": NUM_EPOCHS\n",
    "#     }\n",
    "#     with open(metap, \"w\") as f:\n",
    "#         json.dump(meta, f, indent=4)\n",
    "\n",
    "#     # model setup\n",
    "#     model = CombinedRULModel(X.shape[2], X.shape[1], cat_sizes, 128).to(DEVICE)\n",
    "#     crit = get_criterion()\n",
    "#     opt = get_optimizer(model, lr=LR)\n",
    "#     sched = StepLR(opt, step_size=1, gamma=LR_F)\n",
    "\n",
    "#     best = float('inf')\n",
    "#     noimp = 0\n",
    "\n",
    "#     with open(logp, \"w\") as f:\n",
    "#         f.write(\"epoch,train_loss,val_loss,epoch_time,lr,notes\n",
    "# \")\n",
    "\n",
    "#     start_all = time.perf_counter()\n",
    "#     for ep in range(1, NUM_EPOCHS + 1):\n",
    "#         ep_start = time.perf_counter()\n",
    "#         lr_cur = opt.param_groups[0]['lr']\n",
    "\n",
    "#         # train\n",
    "#         model.train()\n",
    "#         tloss = 0\n",
    "#         for xc, xt, yb in tl:\n",
    "#             xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "#             opt.zero_grad()\n",
    "#             l = crit(model(xc, xt), yb)\n",
    "#             l.backward()\n",
    "#             opt.step()\n",
    "#             tloss += l.item() * yb.size(0)\n",
    "#         tloss /= len(tl.dataset)\n",
    "\n",
    "#         # validation\n",
    "#         model.eval()\n",
    "#         vloss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for xc, xt, yb in vl:\n",
    "#                 xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "#                 l = crit(model(xc, xt), yb)\n",
    "#                 vloss += l.item() * yb.size(0)\n",
    "#         vloss /= len(vl.dataset)\n",
    "\n",
    "#         # timing and checkpoint logic\n",
    "#         elapsed = time.perf_counter() - ep_start\n",
    "#         h, m, s = map(int, [elapsed//3600, (elapsed%3600)//60, elapsed%60])\n",
    "#         et = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "#         notes = \"\"\n",
    "#         if vloss < best:\n",
    "#             best = vloss\n",
    "#             noimp = 0\n",
    "#             torch.save(model.state_dict(), ckpt)\n",
    "#             notes = f\"Model saved at epoch {ep}\"\n",
    "#         else:\n",
    "#             noimp += 1\n",
    "#             if noimp % LR_PAT == 0:\n",
    "#                 sched.step()\n",
    "#                 notes += \" LR stepped\"\n",
    "#             if noimp >= ES_PAT:\n",
    "#                 notes += \" Early stopping\"\n",
    "\n",
    "#         with open(logp, \"a\") as f:\n",
    "#             f.write(f\"{ep},{tloss:.6f},{vloss:.6f},{et},{lr_cur:.6g},{notes.strip()}\n",
    "# \")\n",
    "#         print(f\"Epoch {ep:02d} Train {tloss:.4f} Val {vloss:.4f} Time {et} LR {lr_cur:.2e} {notes}\")\n",
    "\n",
    "#         if noimp >= ES_PAT:\n",
    "#             print(\"Early stop\")\n",
    "#             break\n",
    "\n",
    "#     total = time.perf_counter() - start_all\n",
    "#     h, m, s = map(int, [total//3600, (total%3600)//60, total%60])\n",
    "#     tt = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "#     # update metadata\n",
    "#     with open(metap, \"r+\") as f:\n",
    "#         d = json.load(f)\n",
    "#         d[\"total_training_time\"] = tt\n",
    "#         f.seek(0)\n",
    "#         json.dump(d, f, indent=4)\n",
    "#         f.truncate()\n",
    "\n",
    "#     print(f\"Done. Best val MSE {best:.4f}. Total time {tt}\")\n",
    "\n",
    "# # === Entry Point ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     train(use_h5=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9471b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import h5py\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# === Hard-coded paths ===\n",
    "WINDOW_CSV     = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\super_same_norm.csv\"\n",
    "SPEC_CSV       = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\data\\train_specifications.csv\"\n",
    "ENCODER_PATH   = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\spec_encoder.joblib\"\n",
    "H5_PATH        = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script\\data_windows.h5\"\n",
    "ARTIFACT_ROOT  = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\Important_script2\\artifacts\"\n",
    "\n",
    "# === Utils ===\n",
    "def create_X_y(csv_path=WINDOW_CSV, sensor_features=None, context=70, verbose=True):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y, vids = [], [], []\n",
    "    for vid, grp in df.groupby(\"vehicle_id\"):\n",
    "        data = grp[sensor_features].values\n",
    "        rul = grp[\"RUL\"].values\n",
    "        if len(data) < context:\n",
    "            if verbose:\n",
    "                print(f\"Skipping {vid}, len<{context}\")\n",
    "            continue\n",
    "        for i in range(len(data) - context + 1):\n",
    "            X.append(data[i:i+context])\n",
    "            y.append(rul[i+context-1])\n",
    "            vids.append(vid)\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y)\n",
    "    vids = np.array(vids)\n",
    "    if verbose:\n",
    "        print(f\"Windows: {len(X)}, shape={X.shape[1:]}\")\n",
    "\n",
    "    spec_df = pd.read_csv(SPEC_CSV)\n",
    "    spec_cols = [f\"Spec_{i}\" for i in range(8)]\n",
    "    enc = OrdinalEncoder()\n",
    "    spec_df[spec_cols] = enc.fit_transform(spec_df[spec_cols])\n",
    "    specs = (\n",
    "        pd.DataFrame({\"vehicle_id\": vids})\n",
    "          .merge(spec_df[[\"vehicle_id\"] + spec_cols], on=\"vehicle_id\")\n",
    "    )[spec_cols].values.astype(int)\n",
    "\n",
    "    joblib.dump(enc, ENCODER_PATH)\n",
    "    if verbose:\n",
    "        print(f\"Saved encoder → {ENCODER_PATH}\")\n",
    "\n",
    "    return X, y, vids, specs\n",
    "\n",
    "\n",
    "def save_to_h5(X, y, vids, specs, h5_path=H5_PATH):\n",
    "    with h5py.File(h5_path, \"w\") as f:\n",
    "        f.create_dataset(\"X_windows\", data=X, compression=\"gzip\")\n",
    "        f.create_dataset(\"y_labels\", data=y, compression=\"gzip\")\n",
    "        f.create_dataset(\"window_vids\", data=vids, compression=\"gzip\")\n",
    "        f.create_dataset(\"specs_per_window\", data=specs, compression=\"gzip\")\n",
    "    print(f\"Saved H5 → {h5_path}\")\n",
    "\n",
    "\n",
    "def load_from_h5(h5_path=H5_PATH):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        X = f[\"X_windows\"][:]\n",
    "        y = f[\"y_labels\"][:]\n",
    "        vids = f[\"window_vids\"][:]\n",
    "        specs = f[\"specs_per_window\"][:]\n",
    "    return X, y, vids, specs\n",
    "\n",
    "\n",
    "class RULCombinedDataset(Dataset):\n",
    "    def __init__(self, X, specs, y):\n",
    "        self.X = X\n",
    "        self.specs = specs\n",
    "        self.y = y.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            torch.from_numpy(self.specs[i]).long(),\n",
    "            torch.from_numpy(self.X[i]).float(),\n",
    "            torch.from_numpy(self.y[i]).float()\n",
    "        )\n",
    "\n",
    "\n",
    "def make_artifact_folder(model_name, suffix):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    folder = f\"{model_name}-{suffix}-{ts}\"\n",
    "    path = os.path.join(ARTIFACT_ROOT, folder)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# === Models ===\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        return x[:, -1, :]\n",
    "\n",
    "\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "        super().__init__()\n",
    "        self.tf = TimeSeriesEmbedder(num_sensor_features, continuous_dim)\n",
    "        if cont_mean_std is None:\n",
    "            cont_mean_std = torch.stack([torch.zeros(continuous_dim), torch.ones(continuous_dim)], dim=1)\n",
    "        self.tab = TabTransformer(\n",
    "            categories=categories,\n",
    "            num_continuous=continuous_dim,\n",
    "            dim=continuous_dim,\n",
    "            dim_out=1,\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4,2),\n",
    "            mlp_act=nn.ReLU(),\n",
    "            continuous_mean_std=cont_mean_std\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_ts):\n",
    "        cont = self.tf(x_ts)\n",
    "        return self.tab(x_cat, cont)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a313010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DP Utilities for Spectral-DP ===\n",
    "def spectral_dp_gradient_update(model, sigma, clip_bound):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        grad = param.grad.detach()\n",
    "        if grad.dim() == 2:\n",
    "            U, S, Vh = torch.linalg.svd(grad, full_matrices=False)\n",
    "            S_clipped = torch.clamp(S, max=clip_bound)\n",
    "            noise = torch.randn_like(S_clipped) * sigma * clip_bound\n",
    "            S_noisy = S_clipped + noise\n",
    "            param.grad = (U @ torch.diag(S_noisy) @ Vh).to(param.grad.device)\n",
    "        else:\n",
    "            norm = torch.norm(grad)\n",
    "            factor = min(1.0, clip_bound / (norm + 1e-6))\n",
    "            grad_clipped = grad * factor\n",
    "            noise = torch.randn_like(grad_clipped) * sigma * clip_bound\n",
    "            param.grad = grad_clipped + noise\n",
    "\n",
    "\n",
    "\n",
    "# # import torch\n",
    "\n",
    "# def spectral_dp_gradient_update(model, sigma, clip_bound):\n",
    "#     \"\"\"\n",
    "#     Performs a gradient update using the Spectral-DP method as described in\n",
    "#     arXiv:2307.13231 for all model parameters.\n",
    "\n",
    "#     This implementation reshapes non-2D gradients into 2D matrices to apply\n",
    "#     the spectral perturbation, adhering to the paper's original proposal.\n",
    "\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The model to update.\n",
    "#         sigma (float): The noise multiplier for differential privacy.\n",
    "#         clip_bound (float): The clipping bound for the singular values.\n",
    "#     \"\"\"\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.grad is None:\n",
    "#             continue\n",
    "\n",
    "#         grad = param.grad.detach()\n",
    "#         original_shape = grad.shape\n",
    "\n",
    "#         # --- Reshape non-2D gradients to 2D matrices ---\n",
    "#         if grad.dim() > 2:\n",
    "#             # For convolutional filters (e.g., 4D) or other high-dim tensors,\n",
    "#             # flatten into a 2D matrix. A common way is (out_channels, -1).\n",
    "#             grad_2d = grad.view(original_shape[0], -1)\n",
    "#         elif grad.dim() == 1:\n",
    "#             # For bias vectors (1D), reshape to a column vector (N, 1).\n",
    "#             grad_2d = grad.view(-1, 1)\n",
    "#         else:\n",
    "#             # Gradient is already a 2D matrix.\n",
    "#             grad_2d = grad\n",
    "\n",
    "#         # --- Apply Spectral-DP to the 2D matrix ---\n",
    "#         try:\n",
    "#             # 1. Compute Singular Value Decomposition (SVD)\n",
    "#             U, S, Vh = torch.linalg.svd(grad_2d, full_matrices=False)\n",
    "\n",
    "#             # 2. Clip the singular values\n",
    "#             S_clipped = torch.clamp(S, max=clip_bound)\n",
    "\n",
    "#             # 3. Add noise to the clipped singular values\n",
    "#             # The standard deviation is (sigma * clip_bound)\n",
    "#             noise = torch.randn_like(S_clipped) * sigma * clip_bound\n",
    "#             S_noisy = S_clipped + noise\n",
    "\n",
    "#             # 4. Reconstruct the noisy 2D gradient\n",
    "#             noisy_grad_2d = U @ torch.diag(S_noisy) @ Vh\n",
    "\n",
    "#         except torch.linalg.LinAlgError:\n",
    "#             # SVD can fail on some GPUs for ill-conditioned matrices.\n",
    "#             # In this case, fall back to a standard norm clip and noise on the flattened grad.\n",
    "#             # This is a practical fallback, though not explicitly detailed in the paper.\n",
    "            \n",
    "#             print(f\"SVD failed for {name}. Using standard norm clipping as a fallback.\")\n",
    "\n",
    "            \n",
    "#             norm = torch.norm(grad_2d)\n",
    "#             factor = min(1.0, clip_bound / (norm + 1e-6))\n",
    "#             grad_clipped = grad_2d * factor\n",
    "#             noise = torch.randn_like(grad_clipped) * sigma * clip_bound\n",
    "#             noisy_grad_2d = grad_clipped + noise\n",
    "\n",
    "\n",
    "#         # --- Reshape the gradient back to its original shape and update ---\n",
    "#         param.grad = noisy_grad_2d.reshape(original_shape).to(param.grad.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_criterion(): return MSELoss()\n",
    "def get_optimizer(model,lr=1e-3): return Adam(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Training function ===\n",
    "\n",
    "\n",
    "def train(use_h5=False, pvt=False, sigma=0.1, clip_bound=1.0):# sigma=0.5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    BATCH_SIZE = 256\n",
    "    NUM_EPOCHS = 50\n",
    "    LR = 1e-3\n",
    "    LR_PAT = 5\n",
    "    ES_PAT = 11\n",
    "    LR_F = 0.5\n",
    "\n",
    "    # Load or create data\n",
    "    if use_h5 and os.path.exists(H5_PATH):\n",
    "        X, y, _, specs = load_from_h5()\n",
    "    else:\n",
    "        X, y, _, specs = create_X_y(sensor_features=None)\n",
    "        save_to_h5(X, y, _, specs)\n",
    "\n",
    "    encoder = joblib.load(ENCODER_PATH)\n",
    "    cat_sizes = tuple(len(c) for c in encoder.categories_)\n",
    "\n",
    "    Xtr, Xv, str_, sv, yr, yv = train_test_split(X, specs, y, test_size=0.2, random_state=42)\n",
    "    tl = DataLoader(RULCombinedDataset(Xtr, str_, yr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    vl = DataLoader(RULCombinedDataset(Xv, sv, yv), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Determine suffix based on DP flag\n",
    "    suffix = \"DP\" if pvt else \"NDP\"\n",
    "\n",
    "    # Artifacts\n",
    "    art = make_artifact_folder(\"CombinedRULModel\", suffix)\n",
    "    logp = os.path.join(art, \"train_val_log.txt\")\n",
    "    metap = os.path.join(art, \"metadata.json\")\n",
    "    ckpt = os.path.join(art, \"checkpoint.pth\")\n",
    "\n",
    "    # Metadata init\n",
    "    meta = {\n",
    "        \"model_name\": \"CombinedRULModel\",\n",
    "        \"num_sensor_features\": X.shape[2],\n",
    "        \"context_length\": X.shape[1],\n",
    "        \"continuous_dim\": 128,\n",
    "        \"categories\": list(cat_sizes),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LR,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"privacy\": \"Spectral-DP\" if pvt else \"None\",\n",
    "        \"dp_sigma\": sigma if pvt else None,\n",
    "        \"dp_clip_bound\": clip_bound if pvt else None\n",
    "    }\n",
    "    with open(metap, \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)\n",
    "\n",
    "    # Model setup\n",
    "    model = CombinedRULModel(X.shape[2], X.shape[1], cat_sizes, 128).to(DEVICE)\n",
    "    crit = get_criterion()\n",
    "    opt = get_optimizer(model, lr=LR)\n",
    "    sched = StepLR(opt, step_size=1, gamma=LR_F)\n",
    "\n",
    "    best = float('inf')\n",
    "    noimp = 0\n",
    "\n",
    "    with open(logp, \"w\") as f:\n",
    "        f.write(\"epoch,train_loss,val_loss,epoch_time,lr,notes\\n\")\n",
    "\n",
    "    start_all = time.perf_counter()\n",
    "    for ep in range(1, NUM_EPOCHS + 1):\n",
    "        ep_start = time.perf_counter()\n",
    "        lr_cur = opt.param_groups[0]['lr']\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        tloss = 0\n",
    "        for xc, xt, yb in tl:\n",
    "            xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xc, xt), yb)\n",
    "            loss.backward()\n",
    "            if pvt:\n",
    "                spectral_dp_gradient_update(model, sigma, clip_bound)\n",
    "                opt.step()\n",
    "            else:\n",
    "                opt.step()\n",
    "            tloss += loss.item() * yb.size(0)\n",
    "        tloss /= len(tl.dataset)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        vloss = 0\n",
    "        with torch.no_grad():\n",
    "            for xc, xt, yb in vl:\n",
    "                xc, xt, yb = xc.to(DEVICE), xt.to(DEVICE), yb.to(DEVICE)\n",
    "                l = crit(model(xc, xt), yb)\n",
    "                vloss += l.item() * yb.size(0)\n",
    "        vloss /= len(vl.dataset)\n",
    "\n",
    "        # Checkpointing & scheduler\n",
    "        elapsed = time.perf_counter() - ep_start\n",
    "        h, m, s = map(int, [elapsed//3600, (elapsed%3600)//60, elapsed%60])\n",
    "        et = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "        notes = \"\"\n",
    "        if vloss < best:\n",
    "            best = vloss\n",
    "            noimp = 0\n",
    "            torch.save(model.state_dict(), ckpt)\n",
    "            notes = f\"Saved at epoch {ep}\"\n",
    "        else:\n",
    "            noimp += 1\n",
    "            if noimp % LR_PAT == 0:\n",
    "                sched.step()\n",
    "                notes += \" LR stepped\"\n",
    "            if noimp >= ES_PAT:\n",
    "                notes += \" Early stopping\"\n",
    "\n",
    "        with open(logp, \"a\") as f:\n",
    "            f.write(f\"{ep},{tloss:.6f},{vloss:.6f},{et},{lr_cur:.6g},{notes.strip()}\\n\")\n",
    "        print(f\"Epoch {ep:02d} Train {tloss:.4f} Val {vloss:.4f} Time {et} LR {lr_cur:.2e} {notes}\")\n",
    "\n",
    "        if noimp >= ES_PAT:\n",
    "            print(\"Early stop\")\n",
    "            break\n",
    "\n",
    "    total = time.perf_counter() - start_all\n",
    "    h, m, s = map(int, [total//3600, (total%3600)//60, total%60])\n",
    "    tt = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "    # Update metadata\n",
    "    with open(metap, \"r+\") as f:\n",
    "        d = json.load(f)\n",
    "        d[\"total_training_time\"] = tt\n",
    "        f.seek(0)\n",
    "        json.dump(d, f, indent=4)\n",
    "        f.truncate()\n",
    "\n",
    "    print(f\"Done. Best val MSE {best:.4f}. Total time {tt}\")\n",
    "\n",
    "# === Entry Point ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pvt=True to enable Spectral-DP\n",
    "    train(use_h5=True, pvt=True, sigma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2884fb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_LinalgBackend.Magma: 2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cuda.preferred_linalg_library(\"magma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb4a69",
   "metadata": {},
   "source": [
    "# MATHEMATICAL LOGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a49ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = tensor([[-0.3850, -0.2868,  0.6279,  0.1938],\n",
      "        [-0.4614, -0.7425, -0.4439, -0.1275],\n",
      "        [-0.3780,  0.3269, -0.4040,  0.7604],\n",
      "        [-0.4445,  0.1387,  0.4663,  0.0595],\n",
      "        [-0.5462,  0.4903, -0.1675, -0.6036]])\n",
      "S = tensor([9.3481, 3.8020, 1.4381, 1.0437])\n",
      "Vh = tensor([[-0.5150, -0.3941, -0.4823, -0.5890],\n",
      "        [-0.7196, -0.2809,  0.4471,  0.4511],\n",
      "        [ 0.3262, -0.6791, -0.4157,  0.5095],\n",
      "        [ 0.3326, -0.5519,  0.6283, -0.4359]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a 4x4 matrix\n",
    "A = torch.tensor([\n",
    "    [3.0, 1.0, 1.0, 2.0],\n",
    "    [4.0, 3.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 3.0, 2.0],\n",
    "    [2.0, 1.0, 2.0, 3.0],\n",
    "    [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "])\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Print components\n",
    "print(\"U =\", U)\n",
    "print(\"S =\", S)\n",
    "print(\"Vh =\", Vh)\n",
    "\n",
    "# You can reconstruct A like this:\n",
    "A_reconstructed = U @ torch.diag(S) @ Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a8a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed A = tensor([[3.0000, 1.0000, 1.0000, 2.0000],\n",
      "        [4.0000, 3.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 3.0000, 2.0000],\n",
      "        [2.0000, 1.0000, 2.0000, 3.0000],\n",
      "        [1.0000, 2.0000, 3.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "A = (U @ torch.diag(S) @ Vh)\n",
    "print(\"Reconstructed A =\", A_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1606cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AᵀA:\n",
      " [[ 25 -15]\n",
      " [-15  25]]\n",
      "AAᵀ:\n",
      " [[16 12]\n",
      " [12 34]]\n",
      "Eigenvalues of AᵀA (S²): [40. 10.]\n",
      "Singular values (S): [6.32455532 3.16227766]\n",
      "Right singular vectors (V):\n",
      " [[-0.70710678 -0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "Left singular vectors (U):\n",
      " [[-0.4472136  -0.89442719]\n",
      " [-0.89442719  0.4472136 ]]\n",
      "Reconstructed A:\n",
      " [[ 4.  0.]\n",
      " [ 3. -5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define a small matrix A (2x2 for simplicity)\n",
    "A = np.array([[4, 0],\n",
    "              [3, -5]])\n",
    "\n",
    "# Step 2: Compute AᵀA and AAᵀ\n",
    "ATA = A.T @ A  # shape (2x2)\n",
    "AAT = A @ A.T  # shape (2x2)\n",
    "\n",
    "print(\"AᵀA:\\n\", ATA)\n",
    "print(\"AAᵀ:\\n\", AAT)\n",
    "\n",
    "# Step 3: Eigen decomposition of AᵀA to get V and S²\n",
    "eigvals_V, V = np.linalg.eigh(ATA)\n",
    "# Singular values = sqrt of eigenvalues\n",
    "singular_values = np.sqrt(np.abs(eigvals_V[::-1]))  # sorted in decreasing order\n",
    "V = V[:, ::-1]  # reverse to match sorted eigenvalues\n",
    "\n",
    "print(\"Eigenvalues of AᵀA (S²):\", eigvals_V[::-1])\n",
    "print(\"Singular values (S):\", singular_values)\n",
    "print(\"Right singular vectors (V):\\n\", V)\n",
    "\n",
    "# Step 4: Get U = (1/S) * A * V\n",
    "S_inv = np.diag(1 / singular_values)\n",
    "U = A @ V @ S_inv\n",
    "print(\"Left singular vectors (U):\\n\", U)\n",
    "\n",
    "# Step 5: Final SVD\n",
    "S = np.diag(singular_values)\n",
    "Vh = V.T\n",
    "\n",
    "# Reconstruct A to verify\n",
    "A_reconstructed = U @ S @ Vh\n",
    "print(\"Reconstructed A:\\n\", A_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffa699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
