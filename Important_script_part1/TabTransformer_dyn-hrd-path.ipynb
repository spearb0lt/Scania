{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681bcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c343d1f9",
   "metadata": {},
   "source": [
    "# dynamic path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer  # make sure this is installed\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0. Dynamic “parent” directory and path helper\n",
    "# ---------------------------------------------------\n",
    "parent = os.path.abspath(\"\")\n",
    "print(parent)  # For debugging; can be removed in production\n",
    "\n",
    "def make_path(relative_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper to join the project’s parent folder with a relative subpath.\n",
    "    Example: make_path(\"data/train_specifications.csv\")\n",
    "    will return \"<absolute_parent>/data/train_specifications.csv\".\n",
    "    \"\"\"\n",
    "    return os.path.join(parent, relative_path)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Prepare your raw data\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def create_X_y(csv_path, sensor_features, context=70, verbose=True):\n",
    "    \"\"\"\n",
    "    Reads the time‐series CSV, slides over each vehicle’s data to form windows of length `context`.\n",
    "    Returns:\n",
    "      - X:  numpy array of shape (N, context, num_features)\n",
    "      - y:  numpy array of shape (N,) of RUL labels\n",
    "      - vids: numpy array of shape (N,) of vehicle_ids, one per window\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y, vids = [], [], []\n",
    "    for vehicle_id, group in df.groupby('vehicle_id'):\n",
    "        group = group.sort_values('time_step')\n",
    "        data = group[sensor_features].values\n",
    "        rul = group['RUL'].values\n",
    "        if len(data) < context:\n",
    "            if verbose:\n",
    "                print(f\"Skipping vehicle {vehicle_id}: {len(data)} < {context}\")\n",
    "            continue\n",
    "        for i in range(len(data) - context + 1):\n",
    "            X.append(data[i : i + context])\n",
    "            y.append(rul[i + context - 1])\n",
    "            vids.append(vehicle_id)\n",
    "    X = np.stack(X)      # (N, context, num_features)\n",
    "    y = np.array(y)      # (N,)\n",
    "    vids = np.array(vids)\n",
    "    print(f\"Total windows: {len(X)}, window shape: {X.shape[1:]}\")\n",
    "    return X, y, vids\n",
    "\n",
    "# Define sensor features exactly as before\n",
    "sensor_features = [\n",
    "    '171_0', '666_0', '427_0', '837_0', '167_0', '167_1', '167_2', '167_3', '167_4',\n",
    "    '167_5', '167_6', '167_7', '167_8', '167_9', '309_0', '272_0', '272_1', '272_2',\n",
    "    '272_3', '272_4', '272_5', '272_6', '272_7', '272_8', '272_9', '835_0', '370_0',\n",
    "    '291_0', '291_1', '291_2', '291_3', '291_4', '291_5', '291_6', '291_7', '291_8',\n",
    "    '291_9', '291_10', '158_0', '158_1', '158_2', '158_3', '158_4', '158_5', '158_6',\n",
    "    '158_7', '158_8', '158_9', '100_0', '459_0', '459_1', '459_2', '459_3', '459_4',\n",
    "    '459_5', '459_6', '459_7', '459_8', '459_9', '459_10', '459_11', '459_12', '459_13',\n",
    "    '459_14', '459_15', '459_16', '459_17', '459_18', '459_19', '397_0', '397_1', '397_2',\n",
    "    '397_3', '397_4', '397_5', '397_6', '397_7', '397_8', '397_9', '397_10', '397_11',\n",
    "    '397_12', '397_13', '397_14', '397_15', '397_16', '397_17', '397_18', '397_19',\n",
    "    '397_20', '397_21', '397_22', '397_23', '397_24', '397_25', '397_26', '397_27',\n",
    "    '397_28', '397_29', '397_30', '397_31', '397_32', '397_33', '397_34', '397_35'\n",
    "]\n",
    "\n",
    "# Replace the hard‐coded path with a dynamic one\n",
    "csv_path = make_path(os.path.join(\"data\", \"super_same_norm.csv\"))\n",
    "X_windows, y_labels, window_vids = create_X_y(csv_path, sensor_features, context=70)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1.1 Load & ordinal‐encode the vehicle specs\n",
    "# ---------------------------------------------------\n",
    "spec_csv_rel = os.path.join(\"data\", \"train_specifications.csv\")\n",
    "spec_df = pd.read_csv(make_path(spec_csv_rel))\n",
    "\n",
    "spec_columns = [f\"Spec_{i}\" for i in range(8)]\n",
    "encoder = OrdinalEncoder()\n",
    "spec_df[spec_columns] = encoder.fit_transform(spec_df[spec_columns])\n",
    "\n",
    "# Build matrix of specs per window\n",
    "specs_per_window = (\n",
    "    pd.DataFrame({\"vehicle_id\": window_vids})\n",
    "      .merge(spec_df[[\"vehicle_id\"] + spec_columns], on=\"vehicle_id\", how=\"left\")\n",
    ")[spec_columns].values.astype(int)\n",
    "# specs_per_window: shape (N, 8)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Define the Dataset\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class RULCombinedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns, for each index:\n",
    "      - x_ts: time-series window, shape (context, num_features)\n",
    "      - x_categ: categorical specs, shape (8,)\n",
    "      - y: RUL label (scalar)\n",
    "    \"\"\"\n",
    "    def __init__(self, windows: np.ndarray, specs: np.ndarray, labels: np.ndarray):\n",
    "        super().__init__()\n",
    "        self.windows = windows            # (N, context, num_features)\n",
    "        self.specs = specs                # (N, 8)\n",
    "        self.labels = labels.reshape(-1, 1)  # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_ts = torch.from_numpy(self.windows[idx]).float()    # (context, num_features)\n",
    "        x_categ = torch.from_numpy(self.specs[idx]).long()    # (8,)\n",
    "        y = torch.from_numpy(self.labels[idx]).float()        # (1,)\n",
    "        return x_categ, x_ts, y\n",
    "\n",
    "# Split into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xc_train, Xc_val, xspec_train, xspec_val, y_train, y_val = train_test_split(\n",
    "    X_windows, specs_per_window, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = RULCombinedDataset(Xc_train, xspec_train, y_train)\n",
    "val_dataset   = RULCombinedDataset(Xc_val,   xspec_val,   y_val)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Define the Combined Model\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes as input a batch of raw sensor‐windows shaped (batch, context_length, num_features),\n",
    "    applies a small TransformerEncoder, and returns a single d_model-dim vector per window by last-step pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Project raw sensor‐dim → d_model\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Optional LayerNorm was commented out in your edit\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, context_length, num_features)\n",
    "        returns: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # 1) Project each time‐step’s features into d_model\n",
    "        x = self.input_proj(x)         # (batch, context, d_model)\n",
    "        # 2) Feed into standard PyTorch TransformerEncoder\n",
    "        x = self.encoder(x)            # (batch, context, d_model)\n",
    "        # 3) “Last‐step” pooling instead of mean\n",
    "        x = x[:, -1, :]                # (batch, d_model)\n",
    "        # 4) Optional nonlinearity (commented out in your edit):\n",
    "        # x = F.tanh(x)\n",
    "        return x\n",
    "\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "        \"\"\"\n",
    "        - num_sensor_features: number of raw sensor channels (e.g., 105)\n",
    "        - context_length: length of each time window (e.g., 70)\n",
    "        - categories: tuple of cardinalities for each categorical spec column (length 8)\n",
    "        - continuous_dim: dimensionality of the embedder’s output (e.g., 128)\n",
    "        - cont_mean_std: tensor of shape (continuous_dim, 2) for TabTransformer normalization;\n",
    "                         if None, we assume no normalization (i.e., mean=0, std=1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 3.1 TimeSeries Embedder\n",
    "        self.tf = TimeSeriesEmbedder(\n",
    "            num_features=num_sensor_features,\n",
    "            d_model=continuous_dim,\n",
    "            n_heads=8,\n",
    "            num_layers=2,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # 3.2 TabTransformer\n",
    "        if cont_mean_std is None:\n",
    "            # Create a (continuous_dim x 2) tensor: mean=0, std=1 for each embedding dimension\n",
    "            cont_mean_std = torch.stack([\n",
    "                torch.zeros(continuous_dim),\n",
    "                torch.ones(continuous_dim)\n",
    "            ], dim=1)\n",
    "\n",
    "        self.tabtf = TabTransformer(\n",
    "            categories=categories,              # e.g. (num_levels_spec0, ..., num_levels_spec7)\n",
    "            num_continuous=continuous_dim,      # 128\n",
    "            dim=continuous_dim,                 # internal TabTransformer dimensionality\n",
    "            dim_out=1,                          # single‐value regression\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4, 2),\n",
    "            mlp_act=nn.ReLU(),\n",
    "            continuous_mean_std=cont_mean_std   # commented out per user’s edit\n",
    "        )\n",
    "\n",
    "        # LayerNorm applied after embedder (user added, but can be toggled)\n",
    "        self.layer_norm = nn.LayerNorm(continuous_dim)\n",
    "\n",
    "    def forward(self, x_cat, x_ts):\n",
    "        \"\"\"\n",
    "        x_cat:  (batch_size, 8)              # each entry is an integer spec index\n",
    "        x_ts:   (batch_size, context_length, num_sensor_features)\n",
    "        returns: (batch_size, 1)             # predicted RUL scalar\n",
    "        \"\"\"\n",
    "        # 1) Compute 128‐dim embedding from raw time window\n",
    "        all_embs = self.tf(x_ts)    # (batch_size, 128)\n",
    "        all_embs = self.layer_norm(all_embs)  # Apply LayerNorm to the embeddings    \n",
    "\n",
    "        # 2) Feed embeddings + categorical specs to TabTransformer\n",
    "        pred = self.tabtf(x_cat, all_embs)      # (batch_size, 1)\n",
    "        return pred\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Instantiate model, loss, optimizer, move to GPU\n",
    "# ---------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Determine the cardinalities of each Spec_i from encoder.categories_\n",
    "category_sizes = tuple(len(encoder.categories_[i]) for i in range(len(spec_columns)))  # e.g. (3, 3, 5, etc.)\n",
    "\n",
    "NUM_SENSOR_FEATURES = len(sensor_features)   # 105\n",
    "CONTEXT_LENGTH = 70\n",
    "EMBED_DIM = 128\n",
    "\n",
    "# cont_mean_std not used (per user’s edit)\n",
    "cont_mean_std_tensor = None\n",
    "\n",
    "model = CombinedRULModel(\n",
    "    num_sensor_features=NUM_SENSOR_FEATURES,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    categories=category_sizes,\n",
    "    continuous_dim=EMBED_DIM,\n",
    "    cont_mean_std=cont_mean_std_tensor\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Training & Validation Loops\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x_cat, x_ts, y in train_loader:\n",
    "        x_cat = x_cat.to(device)           # (batch_size, 8)\n",
    "        x_ts = x_ts.to(device)             # (batch_size, 70, 105)\n",
    "        y = y.to(device)                   # (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_cat, x_ts)         # (batch_size, 1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_one_epoch():\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_ts, y in val_loader:\n",
    "            x_cat = x_cat.to(device)\n",
    "            x_ts = x_ts.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(x_cat, x_ts)\n",
    "            loss = criterion(preds, y)\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Run Training\n",
    "# ---------------------------------------------------\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_mse = train_one_epoch()\n",
    "    val_mse = validate_one_epoch()\n",
    "    print(f\"Epoch {epoch:02d} → Train MSE: {train_mse:.4f} | Val MSE: {val_mse:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    model_save_path = make_path(os.path.join(\"models\", \"best_combined_model.pt\"))\n",
    "    if val_mse < best_val_loss:\n",
    "        best_val_loss = val_mse\n",
    "        # Ensure the “models” directory exists\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(\"Training complete. Best validation MSE:\", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c72b",
   "metadata": {},
   "source": [
    "# hardcoded path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82850c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer  # make sure this is installed\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Prepare your raw data\n",
    "# ----------------------------\n",
    "\n",
    "def create_X_y(csv_path, sensor_features, context=70, verbose=True):\n",
    "    \"\"\"\n",
    "    Reads the time‐series CSV, slides over each vehicle’s data to form windows of length `context`.\n",
    "    Returns:\n",
    "      - X:  numpy array of shape (N, context, num_features)\n",
    "      - y:  numpy array of shape (N,) of RUL labels\n",
    "      - vids: numpy array of shape (N,) of vehicle_ids, one per window\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y, vids = [], [], []\n",
    "    for vehicle_id, group in df.groupby('vehicle_id'):\n",
    "        group = group.sort_values('time_step')\n",
    "        data = group[sensor_features].values\n",
    "        rul = group['RUL'].values\n",
    "        if len(data) < context:\n",
    "            if verbose:\n",
    "                print(f\"Skipping vehicle {vehicle_id}: {len(data)} < {context}\")\n",
    "            continue\n",
    "        for i in range(len(data) - context + 1):\n",
    "            X.append(data[i : i + context])\n",
    "            y.append(rul[i + context - 1])\n",
    "            vids.append(vehicle_id)\n",
    "    X = np.stack(X)      # (N, context, num_features)\n",
    "    y = np.array(y)      # (N,)\n",
    "    vids = np.array(vids)\n",
    "    print(f\"Total windows: {len(X)}, window shape: {X.shape[1:]}\")\n",
    "    return X, y, vids\n",
    "\n",
    "# Define sensor features exactly as you had before\n",
    "sensor_features = [\n",
    "    '171_0', '666_0', '427_0', '837_0', '167_0', '167_1', '167_2', '167_3', '167_4',\n",
    "    '167_5', '167_6', '167_7', '167_8', '167_9', '309_0', '272_0', '272_1', '272_2',\n",
    "    '272_3', '272_4', '272_5', '272_6', '272_7', '272_8', '272_9', '835_0', '370_0',\n",
    "    '291_0', '291_1', '291_2', '291_3', '291_4', '291_5', '291_6', '291_7', '291_8',\n",
    "    '291_9', '291_10', '158_0', '158_1', '158_2', '158_3', '158_4', '158_5', '158_6',\n",
    "    '158_7', '158_8', '158_9', '100_0', '459_0', '459_1', '459_2', '459_3', '459_4',\n",
    "    '459_5', '459_6', '459_7', '459_8', '459_9', '459_10', '459_11', '459_12', '459_13',\n",
    "    '459_14', '459_15', '459_16', '459_17', '459_18', '459_19', '397_0', '397_1', '397_2',\n",
    "    '397_3', '397_4', '397_5', '397_6', '397_7', '397_8', '397_9', '397_10', '397_11',\n",
    "    '397_12', '397_13', '397_14', '397_15', '397_16', '397_17', '397_18', '397_19',\n",
    "    '397_20', '397_21', '397_22', '397_23', '397_24', '397_25', '397_26', '397_27',\n",
    "    '397_28', '397_29', '397_30', '397_31', '397_32', '397_33', '397_34', '397_35'\n",
    "]\n",
    "\n",
    "csv_path = r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\Code\\super_same_norm.csv\"\n",
    "X_windows, y_labels, window_vids = create_X_y(csv_path, sensor_features, context=70)\n",
    "\n",
    "# Load & ordinal‐encode the vehicle specs\n",
    "spec_df = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\SCANIA\\2024-34-2\\2024-34-2\\data\\train_specifications.csv\")\n",
    "spec_columns = [f\"Spec_{i}\" for i in range(8)]\n",
    "encoder = OrdinalEncoder()\n",
    "spec_df[spec_columns] = encoder.fit_transform(spec_df[spec_columns])\n",
    "\n",
    "# Build a matrix of specs per window, aligned with X_windows ordering\n",
    "specs_per_window = (\n",
    "    pd.DataFrame({\"vehicle_id\": window_vids})\n",
    "      .merge(spec_df[[\"vehicle_id\"] + spec_columns], on=\"vehicle_id\", how=\"left\")\n",
    ")[spec_columns].values.astype(int)\n",
    "# specs_per_window: shape (N, 8)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Define the Dataset\n",
    "# ----------------------------\n",
    "\n",
    "class RULCombinedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns, for each index:\n",
    "      - x_ts: time-series window, shape (context, num_features)\n",
    "      - x_categ: categorical specs, shape (8,)\n",
    "      - y: RUL label (scalar)\n",
    "    \"\"\"\n",
    "    def __init__(self, windows: np.ndarray, specs: np.ndarray, labels: np.ndarray):\n",
    "        super().__init__()\n",
    "        self.windows = windows            # (N, context, num_features)\n",
    "        self.specs = specs                # (N, 8)\n",
    "        self.labels = labels.reshape(-1, 1)  # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_ts = torch.from_numpy(self.windows[idx]).float()    # (context, num_features)\n",
    "        x_categ = torch.from_numpy(self.specs[idx]).long()    # (8,)\n",
    "        y = torch.from_numpy(self.labels[idx]).float()        # (1,)\n",
    "        return x_categ, x_ts, y\n",
    "\n",
    "# Split into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xc_train, Xc_val, xspec_train, xspec_val, y_train, y_val = train_test_split(\n",
    "    X_windows, specs_per_window, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = RULCombinedDataset(Xc_train, xspec_train, y_train)\n",
    "val_dataset   = RULCombinedDataset(Xc_val,   xspec_val,   y_val)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Define the Combined Model\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "class TimeSeriesEmbedder(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, n_heads=8, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Project raw sensor‐dim → d_model\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # We will mean‐pool over the time dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, context_length, num_features)\n",
    "        returns: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # 1) Project each time‐step’s features into d_model\n",
    "        x = self.input_proj(x)         # (batch, context, d_model)\n",
    "        # 2) Feed into standard PyTorch TransformerEncoder\n",
    "        x = self.encoder(x)            # (batch, context, d_model)\n",
    "        # 3) Mean‐pool across the context dimension\n",
    "        # x = x.mean(dim=1)              # (batch, d_model)\n",
    "        x=x[:, -1, :]\n",
    "        # x=F.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x: (batch_size, context_length, num_features)\n",
    "x = self.input_proj(x)   # → (batch_size, context_length, d_model)#self.input_proj linearly maps each of the num_features at every time step into a d_model-dimensional vector.\n",
    "x = self.encoder(x)      # → (batch_size, context_length, d_model) #self.encoder (the nn.TransformerEncoder) processes the entire sequence of length context_length and returns another sequence of the same shape:\n",
    "|\n",
    "\\\n",
    "  >  x: (batch_size, context_length, d_model)\n",
    "\n",
    "for each window of 70 time steps, the encoder gives you 70 separate “token” embeddings, \n",
    "each of size d_model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now \n",
    "x has shape (batch_size, context_length, d_model). \n",
    "specifically in my combined model—want a single fixed-size vector per window,\n",
    "not a sequence of 70 vectors. \n",
    "\n",
    "TabTransformer expects its “continuous” input to be a 2D tensor of shape (batch_size, continuous_dim).\n",
    "cannot feed it a 3D tensor of shape (batch_size, context_length, continuous_dim)\n",
    "\n",
    "\n",
    "\n",
    "x = x.mean(dim=1)   # → (batch_size, d_model)\n",
    "“pooling” (averaging) across all time steps. The result is a single d_model-length vector \n",
    "per training example in the batch. That vector is intended to capture, in a coarse way, \n",
    "the entire 70-step window’s information.\n",
    "\n",
    "TransformerEncoder gives you 70 “time-step embeddings” of size d_model.\n",
    "Need a single 128-dimensional embedding for each window.\n",
    "Taking the arithmetic mean along dim=1 is a simple way to collapse the time dimension into one vector.\n",
    "\n",
    "Other common pooling choices might be:\n",
    "*Last time step: x = x[:, -1, :]\n",
    "*Max pooling: x = x.max(dim=1).values\n",
    "*Learned “classification” token: prepend a dummy token and read its embedding (like BERT’s [CLS]).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if i output x just like that then\n",
    "model’s forward would output a 3D tensor (batch_size, 70, 128). \n",
    "The downstream TabTransformer is not designed to accept a 3D input.\n",
    " It expects:\n",
    "*a categorical tensor of shape (batch_size, num_categorical_features)\n",
    "*a continuous tensor of shape (batch_size, num_continuous_features)\n",
    "\n",
    "skip the pooling, we need rework TabTransformer to process 70 separate continuous vectors per example.\n",
    " Shape mismatch—TabTransformer’s signature is forward(x_cat, x_cont) where x_cont must be 2D: (batch, cont_dim).\n",
    "Architectural intent—we want each window summarized by a single embedding. Returning a 70-step sequence defeats that design.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "as peer github this is the code \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "cont_mean_std = torch.randn(10, 2)\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories = (10, 5, 6, 5, 8),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = 10,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1,                   # feed forward dropout\n",
    "    mlp_hidden_mults = (4, 2),          # relative multiples of each hidden dimension of the last mlp to logits\n",
    "    mlp_act = nn.ReLU(),                # activation for final mlp, defaults to relu, but could be anything else (selu etc)\n",
    "    continuous_mean_std = cont_mean_std # (optional) - normalize the continuous values before layer norm\n",
    ")\n",
    "\n",
    "x_categ = torch.randint(0, 5, (1, 5))     # category values, from 0 - max number of categories, in the order as passed into the constructor above\n",
    "x_cont = torch.randn(1, 10)               # assume continuous values are already normalized individually\n",
    "\n",
    "pred = model(x_categ, x_cont) # (1, 1)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CombinedRULModel(nn.Module):\n",
    "    def __init__(self, num_sensor_features, context_length, categories, continuous_dim, cont_mean_std=None):\n",
    "        \"\"\"\n",
    "        - num_sensor_features: number of raw sensor channels (e.g., 105)\n",
    "        - context_length: length of each time window (e.g., 70)\n",
    "        - categories: tuple of cardinalities for each categorical spec column (length 8)\n",
    "        - continuous_dim: dimensionality of the embedder’s output (e.g., 128)\n",
    "        - cont_mean_std: tensor of shape (continuous_dim, 2) for TabTransformer normalization;\n",
    "                         if None, we assume no normalization (i.e., mean=0, std=1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self.layer_norm = nn.LayerNorm(continuous_dim)\n",
    "\n",
    "        # 3.1 TimeSeries Embedder\n",
    "        self.tf = TimeSeriesEmbedder(\n",
    "            num_features=num_sensor_features,\n",
    "            d_model=continuous_dim,\n",
    "            n_heads=8,\n",
    "            num_layers=2,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # 3.2 TabTransformer\n",
    "        # If you want TabTransformer to normalize continuous features, pass cont_mean_std.\n",
    "        # Otherwise, set a trivial mean/std.\n",
    "        if cont_mean_std is None:\n",
    "            # Create a (continuous_dim x 2) tensor: mean=0, std=1 for each embedding dimension\n",
    "            cont_mean_std = torch.stack([\n",
    "                torch.zeros(continuous_dim),\n",
    "                torch.ones(continuous_dim)\n",
    "            ], dim=1)\n",
    "\n",
    "        self.tabtf = TabTransformer(\n",
    "            categories=categories,              # e.g. (num_levels_spec0, ..., num_levels_spec7)\n",
    "            num_continuous=continuous_dim,      # 128\n",
    "            dim=continuous_dim,                 # internal TabTransformer dimensionality\n",
    "            dim_out=1,                          # single‐value regression\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4, 2),\n",
    "            mlp_act=nn.ReLU()\n",
    "            # ,continuous_mean_std=cont_mean_std   # tensor shape (128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_ts):\n",
    "        \"\"\"\n",
    "        x_cat:  (batch_size, 8)              # each entry is an integer spec index\n",
    "        x_ts:   (batch_size, context, F)      # raw sensor window,(256, 70, 105)\n",
    "        returns: (batch_size, 1)             # predicted RUL scalar\n",
    "        \"\"\"\n",
    "        # 1) Compute 128‐dim embedding from raw time window\n",
    "        all_embs = self.tf(x_ts)    # (batch_size, 128)\n",
    "        all_embs= self.layer_norm(all_embs)  # Apply LayerNorm to the embeddings    \n",
    "\n",
    "        # 2) Feed embeddings + categorical specs to TabTransformer\n",
    "        pred = self.tabtf(x_cat, all_embs)      # (batch_size, 1)\n",
    "        return pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407005d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 4. Instantiate model, loss, optimizer, move to GPU\n",
    "# ---------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Determine the cardinalities of each Spec_i from encoder.categories_\n",
    "category_sizes = tuple(len(encoder.categories_[i]) for i in range(len(spec_columns)))  # (e.g. (3, 3, 5, ...))\n",
    "\n",
    "NUM_SENSOR_FEATURES = len(sensor_features)   # 105\n",
    "CONTEXT_LENGTH = 70\n",
    "EMBED_DIM = 128\n",
    "\n",
    "# If you really want TabTransformer to standardize the embeddings internally,\n",
    "# you could do a dummy pass over the train set to compute mean/std of all embeddings.\n",
    "# For now, we'll skip that and let TabTransformer assume (mean=0,std=1).\n",
    "cont_mean_std_tensor = None\n",
    "\n",
    "model = CombinedRULModel(\n",
    "    num_sensor_features=NUM_SENSOR_FEATURES,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    categories=category_sizes,\n",
    "    continuous_dim=EMBED_DIM,\n",
    "    cont_mean_std=cont_mean_std_tensor\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Training & Validation Loops\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x_cat, x_ts, y in train_loader:\n",
    "        x_cat = x_cat.to(device)           # (batch_size, 8)\n",
    "        x_ts = x_ts.to(device)             # (batch_size, 70, 105)\n",
    "        y = y.to(device)                   # (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_cat, x_ts)         # (batch_size, 1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_one_epoch():\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_ts, y in val_loader:\n",
    "            x_cat = x_cat.to(device)\n",
    "            x_ts = x_ts.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(x_cat, x_ts)\n",
    "            loss = criterion(preds, y)\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Run Training\n",
    "# ---------------------------------------------------\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_mse = train_one_epoch()\n",
    "    val_mse = validate_one_epoch()\n",
    "    print(f\"Epoch {epoch:02d} → Train MSE: {train_mse:.4f} | Val MSE: {val_mse:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_mse < best_val_loss:\n",
    "        best_val_loss = val_mse\n",
    "        torch.save(model.state_dict(), \"best_combined_model.pt\")\n",
    "\n",
    "print(\"Training complete. Best validation MSE:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144df97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0075c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YourModelClass()\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))  # use the correct path\n",
    "model.eval()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Visualize weights\n",
    "def visualize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.requires_grad:\n",
    "            weights = param.data.cpu().numpy()\n",
    "\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            if weights.ndim == 2:\n",
    "                sns.heatmap(weights, cmap=\"viridis\", cbar=True)\n",
    "                plt.title(f'Heatmap of {name}')\n",
    "                plt.xlabel(\"Output units\")\n",
    "                plt.ylabel(\"Input units\")\n",
    "            else:\n",
    "                plt.plot(weights)\n",
    "                plt.title(f'Weights of {name}')\n",
    "                plt.xlabel(\"Index\")\n",
    "                plt.ylabel(\"Weight value\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "visualize_weights(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
